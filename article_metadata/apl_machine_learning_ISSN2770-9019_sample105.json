[
    {
        "title": "Pair distribution function analysis for oxide defect identification through feature extraction and supervised learning",
        "doi": "10.1063/5.0130681",
        "year": 2023,
        "abstract": "<jats:p>Feature extraction and a neural network model are applied to predict defect types and concentrations in experimental anatase TiO2 samples. A dataset of TiO2 structures with vacancies and interstitials of oxygen and titanium is built, and the structures are relaxed using energy minimization. The features of the calculated pair distribution functions (PDFs) of these defected structures are extracted using linear methods (principal component analysis and non-negative matrix factorization) and non-linear methods (autoencoder and convolutional neural network). The extracted features are used as inputs to a neural network that maps feature weights to the concentration of each defect type. The performance of this machine learning pipeline is validated by predicting defect concentrations based on experimentally measured TiO2 PDFs and comparing the results to brute-force predictions. A physics-based initialization of the autoencoder has the highest accuracy in predicting defect concentrations. This model incorporates physical interpretability and predictability of material structures, enabling a more efficient characterization process with scattering data.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Leveraging graph neural networks and neural operator techniques for high-fidelity mesh-based physics simulations",
        "doi": "10.1063/5.0167014",
        "year": 2023,
        "abstract": "<jats:p>Developing fast and accurate computational models to simulate intricate physical phenomena has been a persistent research challenge. Recent studies have demonstrated remarkable capabilities in predicting various physical outcomes through machine learning-assisted approaches. However, it remains challenging to generalize current methods, usually crafted for a specific problem, to other more complex or broader scenarios. To address this challenge, we developed graph neural network (GNN) models with enhanced generalizability derived from the distinct GNN architecture and neural operator techniques. As a proof of concept, we employ our GNN models to predict finite element (FE) simulation results for three-dimensional solid mechanics problems with varying boundary conditions. Results show that our GNN model achieves accurate and robust performance in predicting the stress and deformation profiles of structures compared with FE simulations. Furthermore, the neural operator embedded GNN approach enables learning and predicting various solid mechanics problems in a generalizable fashion, making it a promising approach for surrogate modeling.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Random forests for detecting weak signals and extracting physical information: A case study of magnetic navigation",
        "doi": "10.1063/5.0189564",
        "year": 2024,
        "abstract": "<jats:p>It has been recently demonstrated that two machine-learning architectures, reservoir computing and time-delayed feed-forward neural networks, can be exploited for detecting the Earth\u2019s anomaly magnetic field immersed in overwhelming complex signals for magnetic navigation in a GPS-denied environment. The accuracy of the detected anomaly field corresponds to a positioning accuracy in the range of 10\u201340\u00a0m. To increase the accuracy and reduce the uncertainty of weak signal detection as well as to directly obtain the position information, we exploit the machine-learning model of random forests that combines the output of multiple decision trees to give optimal values of the physical quantities of interest. In particular, from time-series data gathered from the cockpit of a flying airplane during various maneuvering stages, where strong background complex signals are caused by other elements of the Earth\u2019s magnetic field and the fields produced by the electronic systems in the cockpit, we demonstrate that the random-forest algorithm performs remarkably well in detecting the weak anomaly field and in filtering the position of the aircraft. With the aid of the conventional inertial navigation system, the positioning error can be reduced to less than 10\u00a0m. We also find that, contrary to the conventional wisdom, the classic Tolles\u2013Lawson model for calibrating and removing the magnetic field generated by the body of the aircraft is not necessary and may even be detrimental for the success of the random-forest method.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Experimental realization of a quantum classification: Bell state measurement via machine learning",
        "doi": "10.1063/5.0149414",
        "year": 2023,
        "abstract": "<jats:p>The Bell state is a crucial resource for the realization of quantum information tasks, and when combined with orbital angular momentum (OAM), it enables a high-dimensional Hilbert space, which is essential for high-capacity quantum communication. In this study, we demonstrate the recognition of OAM Bell states using interference patterns generated by a classical light source and a single-photon source from a Sagnac interferometer-based OAM Bell state evolution device. The interference patterns exhibit a one-to-one correspondence with the input Bell states, providing conclusive evidence for the full recognition of OAM Bell states. Furthermore, we introduce machine learning to the field of Bell state recognition by proposing a neural network model capable of accurately recognizing higher order single-photon OAM Bell states, even in the undersampling case. In particular, the model\u2019s training set includes interference patterns of OAM Bell states generated by classical light sources, yet it is able to recognize single-photon OAM Bell states with high accuracy, without relying on quantum resources during training. Our innovative application of neural networks to the recognition of single-photon OAM Bell states not only circumvents the resource consumption and experimental difficulties associated with quantum light sources but also facilitates the study of OAM-based quantum information.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Analysis of VMM computation strategies to implement BNN applications on RRAM arrays",
        "doi": "10.1063/5.0139583",
        "year": 2023,
        "abstract": "<jats:p>The growing interest in edge-AI solutions and advances in the field of quantized neural networks have led to hardware efficient binary neural networks (BNNs). Extreme BNNs utilize only binary weights and activations, making them more memory efficient. Such networks can be realized using exclusive-NOR (XNOR) gates and popcount circuits. The analog in-memory realization of BNNs utilizing emerging non-volatile memory devices has been widely explored recently. However, most realizations typically use 2T-2R synapses, resulting in sub-optimal area utilization. In this study, we investigate alternate computation mapping strategies to realize BNN using selectorless resistive random access memory arrays. A new differential computation scheme that shows a comparable performance with the well-established XNOR computation strategy is proposed. Through extensive experimental characterization, BNN implementation using a non-filamentary bipolar oxide-based random access memory device-based crossbar is demonstrated for two datasets: (i) experimental characterization was performed on a thermal-image based Rock-Paper-Scissors dataset to analyze the impact of sneak-paths with real-hardware experiments. (ii) Large-scale BNN simulations on the Fashion-MNIST dataset with multi-level cell characteristics of non-filamentary devices are performed to demonstrate the impact of device non-idealities.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Noise tailoring, noise annealing, and external perturbation injection strategies in memristive Hopfield neural networks",
        "doi": "10.1063/5.0173662",
        "year": 2024,
        "abstract": "<jats:p>The commercial introduction of a novel electronic device is often preceded by a lengthy material optimization phase devoted to the suppression of device noise as much as possible. The emergence of novel computing architectures, however, triggers a paradigm shift in noise engineering, demonstrating that non-suppressed but properly tailored noise can be harvested as a computational resource in probabilistic computing schemes. Such a strategy was recently realized on the hardware level in memristive Hopfield neural networks, delivering fast and highly energy efficient optimization performance. Inspired by these achievements, we perform a thorough analysis of simulated memristive Hopfield neural networks relying on realistic noise characteristics acquired on various memristive devices. These characteristics highlight the possibility of orders of magnitude variations in the noise level depending on the material choice as well as on the resistance state (and the corresponding active region volume) of the devices. Our simulations separate the effects of various device non-idealities on the operation of the Hopfield neural network by investigating the role of the programming accuracy as well as the noise-type and noise amplitude of the ON and OFF states. Relying on these results, we propose optimized noise tailoring and noise annealing strategies, comparing the impact of internal noise to the effect of external perturbation injection schemes.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Machine learning based hybrid ensemble models for prediction of organic dyes photophysical properties: Absorption wavelengths, emission wavelengths, and quantum yields",
        "doi": "10.1063/5.0181294",
        "year": 2024,
        "abstract": "<jats:p>Fluorescent organic dyes are extensively used in the design and discovery of new materials, photovoltaic cells, light sensors, imaging applications, medicinal chemistry, drug design, energy harvesting technologies, dye and pigment industries, and pharmaceutical industries, among other things. However, designing and synthesizing new fluorescent organic dyes with desirable properties for specific applications requires knowledge of the chemical and physical properties of previously studied molecules. It is a difficult task for experimentalists to identify the photophysical properties of the required chemical molecule at negligible time and financial cost. For this purpose, machine learning-based models are a highly demanding technique for estimating photophysical properties and may be an alternative approach to density functional theory. In this study, we used 15 single models and proposed three different hybrid models to assess a dataset of 3066 organic materials for predicting photophysical properties. The performance of these models was evaluated using three evaluation parameters: mean absolute error, root mean squared error, and the coefficient of determination (R2) on the test-size data. All the proposed hybrid models achieved the highest accuracy (R2) of 97.28%, 95.19%, and 74.01% for predicting the absorption wavelengths, emission wavelengths, and quantum yields, respectively. These resultant outcomes of the proposed hybrid models are \u223c1.9%, \u223c2.7%, and \u223c2.4% higher than the recently reported best models\u2019 values in the same dataset for absorption wavelengths, emission wavelengths, and quantum yields, respectively. This research promotes the quick and accurate production of new fluorescent organic dyes with desirable photophysical properties for specific applications.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "A physics-based predictive model for pulse design to realize high-performance memristive neural networks",
        "doi": "10.1063/5.0180346",
        "year": 2023,
        "abstract": "<jats:p>Memristive neural networks have extensively been investigated for their capability in handling various artificial intelligence tasks. The training performance of memristive neural networks depends on the pulse scheme applied to the constituent memristors. However, the design of the pulse scheme in most previous studies was approached in an empirical manner or through a trial-and-error method. Here, we choose ferroelectric tunnel junction (FTJ) as a model memristor and demonstrate a physics-based predictive model for the pulse design to achieve high training performance. This predictive model comprises a physical model for FTJ that can adequately describe the polarization switching and memristive switching behaviors of the FTJ and an FTJ-based neural network that uses the long-term potentiation (LTP)/long-term depression (LTD) characteristics of the FTJ for the weight update. Simulation results based on the predictive model demonstrate that the LTP/LTD characteristics with a good trade-off between ON/OFF ratio, nonlinearity, and asymmetry can lead to high training accuracies for the FTJ-based neural network. Moreover, it is revealed that an amplitude-increasing pulse scheme may be the most favorable pulse scheme as it offers the widest ranges of pulse amplitudes and widths for achieving high accuracies. This study may provide useful guidance for the pulse design in the experimental development of high-performance memristive neural networks.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Accelerating defect predictions in semiconductors using graph neural networks",
        "doi": "10.1063/5.0176333",
        "year": 2024,
        "abstract": "<jats:p>First-principles computations reliably predict the energetics of point defects in semiconductors but are constrained by the expense of using large supercells and advanced levels of theory. Machine learning models trained on computational data, especially ones that sufficiently encode defect coordination environments, can be used to accelerate defect predictions. Here, we develop a framework for the prediction and screening of native defects and functional impurities in a chemical space of group IV, III\u2013V, and II\u2013VI zinc blende semiconductors, powered by crystal Graph-based Neural Networks (GNNs) trained on high-throughput density functional theory (DFT) data. Using an innovative approach of sampling partially optimized defect configurations from DFT calculations, we generate one of the largest computational defect datasets to date, containing many types of vacancies, self-interstitials, anti-site substitutions, impurity interstitials and substitutions, as well as some defect complexes. We applied three types of established GNN techniques, namely crystal graph convolutional neural network, materials graph network, and Atomistic Line Graph Neural Network (ALIGNN), to rigorously train models for predicting defect formation energy (DFE) in multiple charge states and chemical potential conditions. We find that ALIGNN yields the best DFE predictions with root mean square errors around 0.3\u00a0eV, which represents a prediction accuracy of 98% given the range of values within the dataset, improving significantly on the state-of-the-art. We further show that GNN-based defective structure optimization can take us close to DFT-optimized geometries at a fraction of the cost of full DFT. The current models are based on the semi-local generalized gradient approximation-Perdew\u2013Burke\u2013Ernzerhof (PBE) functional but are highly promising because of the correlation of computed energetics and defect levels with higher levels of theory and experimental data, the accuracy and necessity of discovering novel metastable and low energy defect structures at the PBE level of theory before advanced methods could be applied, and the ability to train multi-fidelity models in the future with new data from non-local functionals. The DFT-GNN models enable prediction and screening across thousands of hypothetical defects based on both unoptimized and partially optimized defective structures, helping identify electronically active defects in technologically important semiconductors.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Deep learning-enabled probing of irradiation-induced defects in time-series micrographs",
        "doi": "10.1063/5.0186046",
        "year": 2024,
        "abstract": "<jats:p>Modeling time-series data with convolutional neural networks (CNNs) requires building a model to learn in batches as opposed to training sequentially. Coupling CNNs with in situ or operando techniques opens the possibility of accurately segmenting dynamic reactions and mass transport phenomena to understand how materials behave under the conditions in which they are used. In this article, in situ ion irradiation transmission electron microscopy (TEM) images are used as inputs into the CNN to assess the defect generation rate, defect cluster density, and saturation of defects. We then use the output segmentation maps to correlate with conventional TEM micrographs to assess the model\u2019s ability to detail nanoscale interactions. Next, we discuss the implications of preprocessing and hyperparameters on model variability, accuracy when expanded to other datasets, and the role of regularization when controlling model variance. Ultimately, we eliminate human bias when extrapolating physical metrics, speed up analysis time, decouple reactions that happen at 100\u00a0ms intervals, and deploy models that are both accurate and transferable to similar experiments.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Discovery of structure\u2013property relations for molecules via hypothesis-driven active learning over the chemical space",
        "doi": "10.1063/5.0157644",
        "year": 2023,
        "abstract": "<jats:p>The discovery of the molecular candidates for application in drug targets, biomolecular systems, catalysts, photovoltaics, organic electronics, and batteries necessitates the development of machine learning algorithms capable of rapid exploration of chemical spaces targeting the desired functionalities. Here, we introduce a novel approach for active learning over the chemical spaces based on hypothesis learning. We construct the hypotheses on the possible relationships between structures and functionalities of interest based on a small subset of data followed by introducing them as (probabilistic) mean functions for the Gaussian process. This approach combines the elements from the symbolic regression methods, such as SISSO and active learning, into a single framework. The primary focus of constructing this framework is to approximate physical laws in an active learning regime toward a more robust predictive performance, as traditional evaluation on hold-out sets in machine learning does not account for out-of-distribution effects which may lead to a complete failure on unseen chemical space. Here, we demonstrate it for the QM9 dataset, but it can be applied more broadly to datasets from both domains of molecular and solid-state materials sciences.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Deep language models for interpretative and predictive materials science",
        "doi": "10.1063/5.0134317",
        "year": 2023,
        "abstract": "<jats:p>Machine learning (ML) has emerged as an indispensable methodology to describe, discover, and predict complex physical phenomena that efficiently help us learn underlying functional rules, especially in cases when conventional modeling approaches cannot be applied. While conventional feedforward neural networks are typically limited to performing tasks related to static patterns in data, recursive models can both work iteratively based on a changing input and discover complex dynamical relationships in the data. Deep language models can model flexible modalities of data and are capable of learning rich dynamical behaviors as they operate on discrete or continuous symbols that define the states of a physical system, yielding great potential toward end-to-end predictions. Similar to how words form a sentence, materials can be considered as a self-assembly of physically interacted building blocks, where the emerging functions of materials are analogous to the meaning of sentences. While discovering the fundamental relationships between building blocks and function emergence can be challenging, language models, such as recurrent neural networks and long-short term memory networks, and, in particular, attention models, such as the transformer architecture, can solve many such complex problems. Application areas of such models include protein folding, molecular property prediction, prediction of material failure of complex nonlinear architected materials, and also generative strategies for materials discovery. We outline challenges and opportunities, especially focusing on extending the deep-rooted kinship of humans with symbolism toward generalizable artificial intelligence (AI) systems using neuro-symbolic AI, and outline how tools such as ChatGPT and DALL\u00b7E can drive materials discovery.</jats:p>",
        "is_referenced_by_count": 24
    },
    {
        "title": "Stoichiometric growth of SrTiO3 films via Bayesian optimization with adaptive prior mean",
        "doi": "10.1063/5.0132768",
        "year": 2023,
        "abstract": "<jats:p>Perovskite insulator SrTiO3 (STO) is expected to be applied to the next generation of electronic and photonic devices as high-k capacitors and photocatalysts. However, reproducible growth of highly insulating stoichiometric (STO) films remains challenging due to the difficulty of precise stoichiometry control in perovskite oxide films. Here, to grow stoichiometric (STO) thin films by fine-tuning multiple growth conditions, we developed a new Bayesian optimization (BO)-based machine learning method that encourages exploration of the search space by varying the prior mean to get out of suboptimal growth condition parameters. Using simulated data, we demonstrate the efficacy of the new BO method, which reproducibly reaches the global best conditions. With the BO method implemented in machine-learning-assisted molecular beam epitaxy (ML-MBE), a highly insulating stoichiometric (STO) film with no absorption in the bandgap was developed in only 44 MBE growth runs. The proposed algorithm provides an efficient experimental design platform that is not as dependent on the experience of individual researchers and will accelerate not only oxide electronics but also various material syntheses.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Hyena neural operator for partial differential equations",
        "doi": "10.1063/5.0177276",
        "year": 2023,
        "abstract": "<jats:p>Numerically solving partial differential equations typically requires fine discretization to resolve necessary spatiotemporal scales, which can be computationally expensive. Recent advances in deep learning have provided a new approach to solving partial differential equations that involves the use of neural operators. Neural operators are neural network architectures that learn mappings between function spaces and have the capability to solve partial differential equations based on data. This study utilizes a novel neural operator called Hyena, which employs a long convolutional filter that is parameterized by a multilayer perceptron. The Hyena operator is an operation that enjoys sub-quadratic complexity and enjoys a global receptive field at the meantime. This mechanism enhances the model\u2019s comprehension of the input\u2019s context and enables data-dependent weight for different partial differential equation instances. To measure how effective the layers are in solving partial differential equations, we conduct experiments on the diffusion\u2013reaction equation and Navier\u2013Stokes equation and compare it with the Fourier neural operator. Our findings indicate that the Hyena neural operator can serve as an efficient and accurate model for learning the partial differential equation solution operator. The data and code used can be found at https://github.com/Saupatil07/Hyena-Neural-Operator.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "AnalogVNN: A fully modular framework for modeling and optimizing photonic neural networks",
        "doi": "10.1063/5.0134156",
        "year": 2023,
        "abstract": "<jats:p>In this paper, we present AnalogVNN, a simulation framework built on PyTorch that can simulate the effects of optoelectronic noise, limited precision, and signal normalization present in photonic neural network accelerators. We use this framework to train and optimize linear and convolutional neural networks with up to nine layers and \u223c1.7 \u00d7 106 parameters, while gaining insights into how normalization, activation function, reduced precision, and noise influence accuracy in analog photonic neural networks. By following the same layer structure design present in PyTorch, the AnalogVNN framework allows users to convert most digital neural network models to their analog counterparts with just a few lines of code, taking full advantage of the open-source optimization, deep learning, and GPU acceleration libraries available through PyTorch.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Automatic identification of edge localized modes in the DIII-D tokamak",
        "doi": "10.1063/5.0134001",
        "year": 2023,
        "abstract": "<jats:p>Fusion power production in tokamaks uses discharge configurations that risk producing strong type I edge localized modes. The largest of these modes will likely increase impurities in the plasma and potentially damage plasma facing components, such as the protective heat and particle divertor. Machine learning-based prediction and control may provide for the automatic detection and mitigation of these damaging modes before they grow too large to suppress. To that end, large labeled datasets are required for the supervised training of machine learning models. We present an algorithm that achieves 97.7% precision when automatically labeling edge localized modes in the large DIII-D tokamak discharge database. The algorithm has no user controlled parameters and is largely robust to tokamak and plasma configuration changes. This automatically labeled database of events can subsequently feed future training of machine learning models aimed at autonomous edge localized mode control and suppression.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "High-speed CMOS-free purely spintronic asynchronous recurrent neural network",
        "doi": "10.1063/5.0129006",
        "year": 2023,
        "abstract": "<jats:p>The exceptional capabilities of the human brain provide inspiration for artificially intelligent hardware that mimics both the function and the structure of neurobiology. In particular, the recent development of nanodevices with biomimetic characteristics promises to enable the development of neuromorphic architectures with exceptional computational efficiency. In this work, we propose biomimetic neurons comprised of domain wall-magnetic tunnel junctions that can be integrated into the first trainable CMOS-free recurrent neural network with biomimetic components. This paper demonstrates the computational effectiveness of this system for benchmark tasks and its superior computational efficiency relative to alternative approaches for recurrent neural networks.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Long-term forecasts of residential energy profiles based on Conv2D and LSTM models (electricity- and gas-based households)",
        "doi": "10.1063/5.0137443",
        "year": 2023,
        "abstract": "<jats:p>For power system operation and expansion of grid-import systems, an accurate forecast model plays an essential role in the better management of household electricity demands. With the aim of finding an accurate forecast model in the proper representation of various household energy profiles, our research objective is centered on the development of a reliable forecast system for a group of 24-household energy consumers. In this energy study, we proposed long-term forecasts of (1) residential energy profiles within the multi-classification framework and (2) energy costing of the household demands using the Keras two-dimensional convolutional neural network (Conv2D) model and long short-term memory (LSTM) models. These high-level Keras neural networks are built to extract multivariate features for household energy consumption modeling and forecasting. The proposed forecast systems utilized a similar model hyperparameter configuration, while the forecast skills are validated with spatial\u2013temporal variation datasets of ten remote locations. The actual costs of household demand and supply are estimated and compared with Conv2D predictions. The finding results (hourly and seasonal predictions and model evaluation) revealed that Conv2D and LSTM forecast systems are promising for household energy forecast solutions. Experimental results of the Conv2D predictive system achieved better forecast skills [correlation coefficient (0.727\u20130.994) and root mean square error (0.190\u20130.868)] than LSTM forecasts (0.308\u20130.987 and 0.278\u20131.212). However, experimental findings revealed that forecast skills of the predictive systems in residential energy demand predictions are highly influenced by the (1) quality of input datasets, (2) model hyperparameter tuning approach, and (3) learning rate of selected network optimizer(s).</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "A deep learning approach for gas sensor data regression: Incorporating surface state model and GRU-based model",
        "doi": "10.1063/5.0160983",
        "year": 2024,
        "abstract": "<jats:p>Metal\u2013oxide\u2013semiconductor (MOS) gas sensors are widely used for gas detection and monitoring. However, MOS gas sensors have always suffered from instability in the link between gas sensor data and the measured gas concentration. In this paper, we propose a novel deep learning approach that combines the surface state model and a Gated Recurrent Unit (GRU)-based regression to enhance the analysis of gas sensor data. The surface state model provides valuable insights into the microscopic surface processes underlying the conductivity response to pulse heating, while the GRU model effectively captures the temporal dependencies present in time-series data. The experimental results demonstrate that the theory guided model GRU+\u03b2 outperforms the elementary GRU algorithm in terms of accuracy and astringent speed. The incorporation of the surface state model and the parameter rate enhances the model\u2019s accuracy and provides valuable information for learning pulse-heated regression tasks with better generalization. This research exhibits superiority of integrating domain knowledge and deep learning techniques in the field of gas sensor data analysis. The proposed approach offers a practical framework for improving the understanding and prediction of gas concentrations, facilitating better decision-making in various practical applications.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Pulse-stream impact on recognition accuracy of reservoir computing from SiO2-based low power memory devices",
        "doi": "10.1063/5.0131524",
        "year": 2023,
        "abstract": "<jats:p>Reservoir computing (RC)-based neuromorphic applications exhibit extremely low power consumption, thus challenging the use of deep neural networks in terms of both consumption requirements and integration density. Under this perspective, this work focuses on the basic principles of RC systems. The ability of self-selective conductive-bridging random access memory devices to operate in two modes, namely, volatile and non-volatile, by regulating the applied voltage is first presented. We then investigate the relaxation time of these devices as a function of the applied amplitude and pulse duration, a critical step in determining the desired non-linearity by the reservoir. Moreover, we present an in-depth study of the impact of selecting the appropriate pulse-stream and its final effects on the total power consumption and recognition accuracy in a handwritten digit recognition application from the National Institute of Standards and Technology dataset. Finally, we conclude at the optimal pulse-stream of 3-bit, through the minimization of two cost criteria, with the total power remaining at 287 \u00b5W and simultaneously achieving 82.58% recognition accuracy upon the test set.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "VERI-D: A new dataset and method for multi-camera vehicle re-identification of damaged cars under varying lighting conditions",
        "doi": "10.1063/5.0183408",
        "year": 2024,
        "abstract": "<jats:p>Vehicle re-identification (V-ReID) is a critical task that aims to match the same vehicle across images from different camera viewpoints. The previous studies have leveraged attribute clues, such as color, model, and license plate, to enhance the V-ReID performance. However, these methods often lack effective interaction between the global\u2013local features and the final V-ReID objective. Moreover, they do not address the challenging issues in real-world scenarios, such as high viewpoint variations, extreme illumination conditions, and car appearance changes (e.g., due to damage or wrong driving). We propose a novel framework to tackle these problems and advance the research in V-ReID, which can handle various types of car appearance changes and achieve robust V-ReID under varying lighting conditions. Our main contributions are as follows: (i) we propose a new Re-ID architecture named global\u2013local self-attention network, which integrates local information into the feature learning process and enhances the feature representation for V-ReID and (ii) we introduce a novel damaged vehicle Re-ID dataset called VERI-D, which is the first publicly available dataset that focuses on this challenging yet practical scenario. The dataset contains both natural and synthetic images of damaged vehicles captured from multiple camera viewpoints and under different lighting conditions. (iii) We conduct extensive experiments on the VERI-D dataset and demonstrate the effectiveness of our approach in addressing the challenges associated with damaged vehicle re-identification. We also compare our method to several state-of-the-art V-ReID methods and show its superiority.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Impact of analog memory device failure on in-memory computing inference accuracy",
        "doi": "10.1063/5.0131797",
        "year": 2023,
        "abstract": "<jats:p>In-memory computing using analog non-volatile memory (NVM) devices can improve the speed and reduce the latency of deep neural network (DNN) inference. It has been recently shown that neuromorphic crossbar arrays, where each weight is implemented using analog conductance values of phase-change memory devices, achieve competitive accuracy and high power efficiency. However, due to the large amount of NVMs needed and the challenge for making analog NVM devices, these chips typically include some failed devices from fabrication or developed over time. We study the impact of these failed devices on the analog in-memory computing accuracy for various networks. We show that larger networks with fewer reused layers are more tolerable to failed devices. Devices stuck at high resistance states are more tolerable than devices stuck at low resistance states. To improve the robustness of DNNs to defective devices, we develop training methods that add noise and corrupt devices in the weight matrices during network training and show that this can increase the network accuracy in the presence of the failed devices. We also provide estimated maximum defective device tolerance of some common networks.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Rotationally equivariant super-resolution of velocity fields in two-dimensional flows using convolutional neural networks",
        "doi": "10.1063/5.0132326",
        "year": 2023,
        "abstract": "<jats:p>This paper investigates the super-resolution of velocity fields in two-dimensional flows from the viewpoint of rotational equivariance. Super-resolution refers to techniques that enhance the resolution of an image from low to high resolution, and it has recently been applied in fluid mechanics. Rotational equivariance of super-resolution models is defined as the property by which the super-resolved velocity field is rotated according to a rotation of the input, leading to inferences that are covariant with the orientation of fluid systems. In physics, covariance is often related to symmetries. To better understand the connection with symmetries, the notion of rotational consistency of datasets is introduced within the framework of supervised learning, which is defined as the invariance of pairs of low- and high-resolution velocity fields with respect to rotation. This consistency is sufficient and necessary for super-resolution models to learn rotational equivariance from large datasets. Such a large dataset is not required when rotational equivariance is imposed on super-resolution models through the use of prior knowledge in the form of equivariant kernel patterns. Nonetheless, even if a fluid system has rotational symmetry, this symmetry may not carry over to a velocity dataset, which is not rotationally consistent. This inconsistency can arise when the rotation does not commute with the generation of low-resolution velocity fields. These theoretical assertions are supported by the results of numerical experiments, where two existing convolutional neural networks (CNNs) are converted into rotationally equivariant CNNs and the inferences of these CNNs are compared after the supervised training.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "A unifying perspective on non-stationary kernels for deeper Gaussian processes",
        "doi": "10.1063/5.0176963",
        "year": 2024,
        "abstract": "<jats:p>The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning (ML) in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably, the most important building block of a GP is the kernel function, which assumes the role of a covariance operator. Stationary kernels of the Mat\u00e9rn class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more complicated functional form and the associated effort and expertise needed to define and tune them optimally. In this perspective, we want to help ML practitioners make sense of some of the most common forms of non-stationarity for Gaussian processes. We show a variety of kernels in action using representative datasets, carefully study their properties, and compare their performances. Based on our findings, we propose a new kernel that combines some of the identified advantages of existing kernels.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Bring memristive in-memory computing into general-purpose machine learning: A perspective",
        "doi": "10.1063/5.0167743",
        "year": 2023,
        "abstract": "<jats:p>In-memory computing (IMC) using emerging nonvolatile devices has received considerable attention due to its great potential for accelerating artificial neural networks and machine learning tasks. As the basic concept and operation modes of IMC are now well established, there is growing interest in employing its wide and general application. In this perspective, the path that leads memristive IMC to general-purpose machine learning is discussed in detail. First, we reviewed the development timeline of machine learning algorithms that employ memristive devices, such as resistive random-access memory and phase-change memory. Then we summarized two typical aspects of realizing IMC-based general-purpose machine learning. One involves a heterogeneous computing system for algorithmic completeness. The other is to obtain the configurable precision techniques for the compromise of the precision-efficiency dilemma. Finally, the major directions and challenges of memristive IMC-based general-purpose machine learning are proposed from a cross-level design perspective.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Deep ensemble inverse model for image-based estimation of solar cell parameters",
        "doi": "10.1063/5.0139707",
        "year": 2023,
        "abstract": "<jats:p>Physical models can help improve solar cell efficiency during the design phase and for quality control after the fabrication process. We present a data-driven approach to inverse modeling that can predict the underlying parameters of a finite element method solar cell model based on an electroluminescence (EL) image of a solar cell with known cell geometry and laser scribed defects. For training the inverse model, 75\u2009000 synthetic EL images were generated with randomized parameters of the physical cell model. We combine 17 deep convolutional neural networks based on a modified VGG19 architecture into a deep ensemble to add uncertainty estimates. Using the silicon solar cell model, we show that such a novel approach to data-driven statistical inverse modeling can help apply recent developments in deep learning to new engineering applications that require real-time parameterizations of physical models augmented by confidence intervals. The trained network was tested on four different physical solar cell samples, and the estimated parameters were used to create the corresponding model representations. Resimulations of the measurements yielded relative deviations of the calculated and the measured junction voltage values of 0.2% on average with a maximum of 10%, demonstrating the validity of the approach.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Study of the adsorption sites of high entropy alloys for CO2 reduction using graph convolutional network",
        "doi": "10.1063/5.0198043",
        "year": 2024,
        "abstract": "<jats:p>Carbon dioxide reduction is a major step toward building a cleaner and safer environment. There is a surge of interest in exploring high-entropy alloys (HEAs) as active catalysts for CO2 reduction; however, so far, it is mainly limited to quinary HEAs. Inspired by the successful synthesis of octonary and denary HEAs, herein, the CO2 reduction reaction (CO2RR) performance of an HEA composed of Ag, Au, Cu, Pd, Pt, Co, Ga, Ni, and Zn is studied by developing a high-fidelity graph neural network (GNN) framework. Within this framework, the adsorption site geometry and physics are employed through the featurization of elements. Particularly, featurization is performed using various intrinsic properties, such as electronegativity and atomic radius, to enable not only the supervised learning of CO2RR performance descriptors, namely, CO and H adsorption energies, but also the learning of adsorption physics and generalization to unseen metals and alloys. The developed model evaluates the adsorption strength of \u223c3.5 and \u223c0.4 billion possible sites for CO and H, respectively. Despite the enormous space of the AgAuCuPdPtCoGaNiZn alloy and the rather small size of the training data, the GNN framework demonstrated high accuracy and good robustness. This study paves the way for the rapid screening and intelligent synthesis of CO2RR-active and selective HEAs.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Learning thermodynamically constrained equations of state with uncertainty",
        "doi": "10.1063/5.0165298",
        "year": 2024,
        "abstract": "<jats:p>Numerical simulations of high energy-density experiments require equation of state (EOS) models that relate a material\u2019s thermodynamic state variables\u2014specifically pressure, volume/density, energy, and temperature. EOS models are typically constructed using a semi-empirical parametric methodology, which assumes a physics-informed functional form with many tunable parameters calibrated using experimental/simulation data. Since there are inherent uncertainties in the calibration data (parametric uncertainty) and the assumed functional EOS form (model uncertainty), it is essential to perform uncertainty quantification (UQ) to improve confidence in EOS predictions. Model uncertainty is challenging for UQ studies since it requires exploring the space of all possible physically consistent functional forms. Thus, it is often neglected in favor of parametric uncertainty, which is easier to quantify without violating thermodynamic laws. This work presents a data-driven machine learning approach to constructing EOS models that naturally captures model uncertainty while satisfying the necessary thermodynamic consistency and stability constraints. We propose a novel framework based on physics-informed Gaussian process regression (GPR) that automatically captures total uncertainty in the EOS and can be jointly trained on both simulation and experimental data sources. A GPR model for the shock Hugoniot is derived, and its uncertainties are quantified using the proposed framework. We apply the proposed model to learn the EOS for the diamond solid state of carbon using both density functional theory data and experimental shock Hugoniot data to train the model and show that the prediction uncertainty is reduced by considering thermodynamic constraints.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Accelerating the design and development of polymeric materials via deep learning: Current status and future challenges",
        "doi": "10.1063/5.0131067",
        "year": 2023,
        "abstract": "<jats:p>The design and development of polymeric materials have been a hot domain for decades. However, traditional experiments and molecular simulations are time-consuming and labor-intensive, which no longer meet the requirements of new materials development. With the rapid advances of artificial intelligence and materials informatics, machine learning algorithms are increasingly applied in materials science, aiming to shorten the development period of new materials. With the evolution of polymeric materials, the structure of polymers has become more and more complex. Traditional machine learning algorithms often do not perform satisfactorily when dealing with complex data. Presently, deep learning algorithms, including deep neural networks, convolutional neural networks, generative adversarial networks, recurrent neural networks, and graph neural networks, show their uniquely excellent learning capabilities for large and complex data, which will be a powerful tool for the design and development of polymeric materials. This Review introduces principles of several currently popular deep learning algorithms and discusses their multiple applications in the materials field. Applications range from property prediction and molecular generation at the molecular level to structure identification and material synthesis in polymers. Finally, future challenges and opportunities for the application of deep learning in polymeric materials are discussed.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Simulation-free determination of microstructure representative volume element size via Fisher scores",
        "doi": "10.1063/5.0195232",
        "year": 2024,
        "abstract": "<jats:p>A representative volume element (RVE) is a reasonably small unit of microstructure that can be simulated to obtain the same effective properties as the entire microstructure sample. Finite element (FE) simulation of RVEs, as opposed to much larger samples, saves computational expenses, especially in multiscale modeling. Therefore, it is desirable to have a framework that determines the RVE size prior to FE simulations. Existing methods select the RVE size based on when the FE-simulated properties of samples of increasing sizes converge with insignificant statistical variations, with the drawback being that many samples must be simulated. We propose a simulation-free alternative that determines the RVE size based only on a micrograph. The approach utilizes a machine learning model trained to implicitly characterize the stochastic nature of the input micrograph. The underlying rationale is to view RVE size as the smallest moving window size for which the stochastic nature of the microstructure within the window is stationary as the window moves across a large micrograph. For this purpose, we adapt a recently developed Fisher score-based framework for microstructure nonstationarity monitoring. Because the resulting RVE size is based solely on the micrograph and does not involve any FE simulation of specific properties, it constitutes an RVE for any property of interest that solely depends on the microstructure characteristics. Through numerical experiments of simple and complex microstructures, we validate our approach and show that our selected RVE sizes are consistent with when the chosen FE-simulated properties converge.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Deep learning of nonlinear flame fronts development due to Darrieus\u2013Landau instability",
        "doi": "10.1063/5.0139857",
        "year": 2023,
        "abstract": "<jats:p>The Darrieus\u2013Landau instability is studied using a data-driven, deep neural network approach. The task is set up to learn a time-advancement operator mapping any given flame front to a future time. A recurrent application of such an operator rolls out a long sequence of predicted flame fronts, and a learned operator is required to not only make accurate short-term predictions but also reproduce characteristic nonlinear behavior, such as fractal front structures and detached flame pockets. Using two datasets of flame front solutions obtained from a heavy-duty direct numerical simulation and a light-duty modeling equation, we compare the performance of three state-of-art operator-regression network methods: convolutional neural networks, Fourier neural operator (FNO), and deep operator network. We show that, for learning complicated front evolution, FNO gives the best recurrent predictions in both the short and long term. A consistent extension allowing the operator-regression networks to handle complicated flame front shape is achieved by representing the latter as an implicit curve.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "A machine learning framework for elastic constants predictions in multi-principal element alloys",
        "doi": "10.1063/5.0129928",
        "year": 2023,
        "abstract": "<jats:p>On the one hand, multi-principal element alloys (MPEAs) have created a paradigm shift in alloy design due to large compositional space, whereas on the other, they have presented enormous computational challenges for theory-based materials design, especially density functional theory (DFT), which is inherently computationally expensive even for traditional dilute alloys. In this paper, we present a machine learning framework, namely PREDICT (PRedict properties from Existing Database In Complex alloys Territory), that opens a pathway to predict elastic constants in large compositional space with little computational expense. The framework only relies on the DFT database of binary alloys and predicts Voigt\u2013Reuss\u2013Hill Young\u2019s modulus, shear modulus, bulk modulus, elastic constants, and Poisson\u2019s ratio in MPEAs. We show that the key descriptors of elastic constants are the A\u2013B bond length and cohesive energy. The framework can predict elastic constants in hypothetical compositions as long as the constituent elements are present in the database, thereby enabling property exploration in multi-compositional systems. We illustrate predictions in a FCC Ni-Cu-Au-Pd-Pt system.</jats:p>",
        "is_referenced_by_count": 9
    },
    {
        "title": "Optimization of a quantum cascade laser cavity for single-spatial-mode operation via machine learning",
        "doi": "10.1063/5.0158204",
        "year": 2023,
        "abstract": "<jats:p>Neural networks, trained with the ADAM algorithm followed by a globally convergent modification to Newton\u2019s method, are developed to predict the threshold gain of the fundamental and first higher-order modes as functions of the refractive-index profile in a quantum cascade laser cavity. The networks are used to optimize the design of a refractive-index profile that provides essentially single-spatial-mode performance in a nominally multi-moded cavity by maximizing the threshold-gain differential between the modes. The use of neural networks allows the optimization to be performed in seconds, instead of days or weeks which would be required if Maxwell\u2019s equations were repeatedly solved to obtain the threshold gains.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Asymmetric CycleGANs for inverse design of photonic metastructures",
        "doi": "10.1063/5.0159264",
        "year": 2023,
        "abstract": "<jats:p>Using deep learning to develop nanophotonic structures has been an active field of research in recent years to reduce the time intensive iterative solutions found in finite-difference time-domain simulations. Existing work has primarily used a specific type of generative network: conditional deep convolutional generative adversarial networks. However, these networks have issues with producing clear optical structures in image files; for example, a large number of images show speckled noise, which often results in non-manufacturable structures. Here, we report the first use of cycle-consistent generative adversarial networks to design nanophotonic structures. This approach significantly reduces the amount of speckled noise present in generated geometric structures and allows shapes to have clear edges. We demonstrate that for a given input reflectance spectra, the system generates designs in the form of images, and a complementary network generates reflectance spectra if an image containing a shape is provided as an input. The results show a higher Frechet Inception Distance score than previous approaches, which indicates that the generated structures are of higher quality and are able to learn nonlinear relationships between both datasets. This method of designing nanophotonics provides alternative avenues for development that are more noise robust while still adhering to desired optical properties.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Robust design of semi-automated clustering models for 4D-STEM datasets",
        "doi": "10.1063/5.0130546",
        "year": 2023,
        "abstract": "<jats:p>Materials discovery and design require characterizing material structures at the nanometer and sub-nanometer scale. Four-Dimensional Scanning Transmission Electron Microscopy (4D-STEM) resolves the crystal structure of materials, but many 4D-STEM data analysis pipelines are not suited for the identification of anomalous and unexpected structures. This work introduces improvements to the iterative Non-Negative Matrix Factorization (NMF) method by implementing consensus clustering for ensemble learning. We evaluate the performance of models during parameter tuning and find that consensus clustering improves performance in all cases and is able to recover specific grains missed by the best performing model in the ensemble. The methods introduced in this work can be applied broadly to materials characterization datasets to aid in the design of new materials.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "DyFraNet: Forecasting and backcasting dynamic fracture mechanics in space and time using a 2D-to-3D deep neural network",
        "doi": "10.1063/5.0135015",
        "year": 2023,
        "abstract": "<jats:p>The dynamics of material failure is a critical phenomenon relevant to a range of scientific and engineering fields, from healthcare to structural materials. We propose a specially designed deep neural network, DyFraNet, which can predict dynamic fracture behaviors by identifying a complete history of fracture propagation\u2014from the onset of cracking, as a crack grows through the material, modeled as a series of frames evolving over time and dependent on each other. Furthermore, the model can not only forecast future fracture processes but also backcast to elucidate past fracture histories. In this scenario, once provided with the outcome of a fracture event, the model will reveal past events that led to this state and can also predict future evolutions of the failure process. By comparing the predicted results with atomistic-level simulations and theory, we show that DyFraNet can capture dynamic fracture mechanics by accurately predicting how cracks develop over time, including measures such as the crack speed, as well as when cracks become unstable. We use Gradient-weighted Class Activation Mapping, Grad-CAM, to interpret how DyFraNet perceives the relationship between geometric conditions and fracture dynamics, and we find that DyFraNet pays special attention to the areas around crack tips that have a critical influence in the early stage of fracture propagation. In later stages, the model pays increased attention to the existing or newly formed damaged regions in the material. The proposed approach offers the potential to accelerate the exploration of dynamical processes in material design against failure and can be adapted for all kinds of dynamical problems.</jats:p>",
        "is_referenced_by_count": 5
    },
    {
        "title": "Scalable wavelength-multiplexing photonic reservoir computing",
        "doi": "10.1063/5.0158939",
        "year": 2023,
        "abstract": "<jats:p>Photonic reservoir computing (PRC) is a special hardware recurrent neural network, which is featured with fast training speed and low training cost. This work shows a wavelength-multiplexing PRC architecture, taking advantage of the numerous longitudinal modes in a Fabry\u2013Perot (FP) semiconductor laser. These modes construct connected physical neurons in parallel, while an optical feedback loop provides interactive virtual neurons in series. We experimentally demonstrate a four-channel wavelength-multiplexing PRC architecture with a total of 80 neurons. The clock rate of the multiplexing PRC reaches as high as 1.0\u00a0GHz, which is four times higher than that of the single-channel case. In addition, it is proved that the multiplexing PRC exhibits a superior performance on the task of signal equalization in an optical fiber communication link. This improved performance is owing to the rich neuron interconnections both in parallel and in series. In particular, this scheme is highly scalable owing to the rich mode resources in FP lasers.</jats:p>",
        "is_referenced_by_count": 4
    },
    {
        "title": "Completeness of atomic structure representations",
        "doi": "10.1063/5.0160740",
        "year": 2024,
        "abstract": "<jats:p>In this paper, we address the challenge of obtaining a comprehensive and symmetric representation of point particle groups, such as atoms in a molecule, which is crucial in physics and theoretical chemistry. The problem has become even more important with the widespread adoption of machine-learning techniques in science, as it underpins the capacity of models to accurately reproduce physical relationships while being consistent with fundamental symmetries and conservation laws. However, some of the descriptors that are commonly used to represent point clouds\u2014 notably those based on discretized correlations of the neighbor density that power most of the existing ML models of matter at the atomic scale\u2014are unable to distinguish between special arrangements of particles in three dimensions. This makes it impossible to machine learn their properties. Atom-density correlations are provably complete in the limit in which they simultaneously describe the mutual relationship between all atoms, which is impractical. We present a novel approach to construct descriptors of finite correlations based on the relative arrangement of particle triplets, which can be employed to create symmetry-adapted models with universal approximation capabilities, and have the resolution of the neighbor discretization as the sole convergence parameter. Our strategy is demonstrated on a class of atomic arrangements that are specifically built to defy a broad class of conventional symmetric descriptors, showing its potential for addressing their limitations.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "A machine learning-based prediction of crystal orientations for multicrystalline materials",
        "doi": "10.1063/5.0138099",
        "year": 2023,
        "abstract": "<jats:p>We established a rapid, low-cost, and accurate technique to measure crystallographic orientations in multicrystalline materials by optical images and machine learning. A long short-term memory neural network was trained with pairs of light reflection patterns and the correct orientations of each grain, successfully predicting orientation with an error median of 8.61\u00b0. The model was improved by diverse data taken from various incident light angles and by data augmentation. When trained on different incident angles, the model was capable of estimating different orientations. This is related to the geometrical configuration of the incident light angles and surface facets of the crystal. The failure in certain orientations is thought to be complemented by supplementary data taken from different incident angles. Combining data from multiple incident angles, we acquired an error median of 4.35\u00b0. Data augmentation was successfully performed, reducing error by an additional 35%. This technique can provide the crystallographic orientations of a 15 \u00d7 15\u00a0cm2 sized wafer in less than 8\u00a0min, while baseline techniques such as electron backscatter diffraction and Laue scanner may take more than 10\u00a0h. The rapid and accurate measurement can accelerate data collection for full-sized ingots, helping us gain a comprehensive understanding of crystal growth. We believe that our technique will contribute to controlling crystalline structure for the fabrication of high-performance materials.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Materials cartography: A forward-looking perspective on materials representation and devising better maps",
        "doi": "10.1063/5.0149804",
        "year": 2023,
        "abstract": "<jats:p>Machine learning (ML) is gaining popularity as a tool for materials scientists to accelerate computation, automate data analysis, and predict materials properties. The representation of input material features is critical to the accuracy, interpretability, and generalizability of data-driven models for scientific research. In this Perspective, we discuss a few central challenges faced by ML practitioners in developing meaningful representations, including handling the complexity of real-world industry-relevant materials, combining theory and experimental data sources, and describing scientific phenomena across timescales and length scales. We present several promising directions for future research: devising representations of varied experimental conditions and observations, the need to find ways to integrate machine learning into laboratory practices, and making multi-scale informatics toolkits to bridge the gaps between atoms, materials, and devices.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Computational synthesis of a new generation of 2D-based perovskite quantum materials",
        "doi": "10.1063/5.0189497",
        "year": 2024,
        "abstract": "<jats:p>Perovskite-based optoelectronic devices have emerged as a promising energy source due to their potential for scalable production. This study introduces \u201cperovskene,\u201d a novel class of 2D materials derived from the ABC3-like perovskites, synthesized via a data-driven, high-throughput computational strategy. We harness machine learning and multitarget deep neural networks to systematically investigate the structure\u2013property relations, paving the way for targeted material design and optimization in fields such as renewable energy, electronics, and catalysis. The characterization of over 1500 synthesized structures shows that more than 500 structures are stable, revealing properties such as ultra-low work function and large magnetic moment, underscoring the potential for advanced technological applications.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Machine learning guided optimal composition selection of niobium alloys for high temperature applications",
        "doi": "10.1063/5.0129528",
        "year": 2023,
        "abstract": "<jats:p>Nickel- and cobalt-based superalloys are commonly used as turbine materials for high-temperature applications. However, their maximum operating temperature is limited to about 1100\u2009\u00b0C. Therefore, to improve turbine efficiency, current research is focused on designing materials that can withstand higher temperatures. Niobium-based alloys can be considered as promising candidates because of their exceptional properties at elevated temperatures. The conventional approach to alloy design relies on phase diagrams and structure\u2013property data of limited alloys and extrapolates this information into unexplored compositional space. In this work, we harness machine learning and provide an efficient design strategy for finding promising niobium-based alloy compositions with high yield and ultimate tensile strength. Unlike standard composition-based features, we use domain knowledge-based custom features and achieve higher prediction accuracy. We apply Bayesian optimization to screen out novel Nb-based quaternary and quinary alloy compositions and find these compositions have superior predicted strength over a range of temperatures. We develop a detailed design flow and include Python programming code, which could be helpful for accelerating alloy design in a limited alloy data regime.</jats:p>",
        "is_referenced_by_count": 5
    },
    {
        "title": "Multivariate Gaussian process surrogates for predicting basic structural parameters of refractory non-dilute random alloys",
        "doi": "10.1063/5.0186045",
        "year": 2024,
        "abstract": "<jats:p>Refractory non-dilute random alloys consist of two or more principal refractory metals with complex interactions that modify their basic structural properties such as lattice parameters and elastic constants. Atomistic simulations (ASs) are an effective method to compute such basic structural parameters. However, accurate predictions from ASs are computationally expensive due to the size and number of atomistic structures required. To reduce the computational burden, multivariate Gaussian process regression (MVGPR) is proposed as a surrogate model that only requires computing a small number of configurations for training. The elemental atom percentage in the hyper-spherical coordinates is demonstrated to be an effective feature for surrogate modeling. An additive approximation of the full MVGPR model is also proposed to further reduce computations. To improve surrogate accuracy, active learning is used to select a small number of alloys to simulate. Numerical studies based on AS data show the accuracy of the surrogate methodology and the additive approximation, as well as the effectiveness and robustness of the active learning for selecting new alloy designs to simulate.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses",
        "doi": "10.1063/5.0155447",
        "year": 2023,
        "abstract": "<jats:p>Extracting information from radio-frequency (RF) signals using artificial neural networks at low energy cost is a critical need for a wide range of applications from radars to health. These RF inputs are composed of multiple frequencies. Here, we show that magnetic tunnel junctions can process analog RF inputs with multiple frequencies in parallel and perform synaptic operations. Using a backpropagation-free method called extreme learning, we classify noisy images encoded by RF signals, using experimental data from magnetic tunnel junctions functioning as both synapses and neurons. We achieve the same accuracy as an equivalent software neural network. These results are a key step for embedded RF artificial intelligence.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Label free identification of different cancer cells using deep learning-based image analysis",
        "doi": "10.1063/5.0141730",
        "year": 2023,
        "abstract": "<jats:p>Cancer diagnostics is an important field of cancer recovery and survival with many expensive procedures needed to administer the correct treatment. Machine Learning (ML) approaches can help with the diagnostic prediction from circulating tumor cells in liquid biopsy or from a primary tumor in solid biopsy. After predicting the metastatic potential from a deep learning model, doctors in a clinical setting can administer a safe and correct treatment for a specific patient. This paper investigates the use of deep convolutional neural networks for predicting a specific cancer cell line as a tool for label free identification. Specifically, deep learning strategies for weight initialization and performance metrics are described, with transfer learning and the accuracy metric utilized in this work. The equipment used for prediction involves brightfield microscopy without the use of chemical labels, advanced instruments, or time-consuming biological techniques, giving an advantage over current diagnostic methods. In the procedure, three different binary datasets of well-known cancer cell lines were collected, each having a difference in metastatic potential. Two different classification models were adopted (EfficientNetV2 and ResNet-50) with the analysis given for each stage in the ML architecture. The training results for each model and dataset are provided and systematically compared. We found that the test set accuracy showed favorable performance for both ML models with EfficientNetV2 accuracy reaching up to 99%. These test results allowed EfficientNetV2 to outperform ResNet-50 at an average percent increase of 3.5% for each dataset. The high accuracy obtained from the predictions demonstrates that the system can be retrained on a large-scale clinical dataset.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Machine learning assisted interpretation of creep and fatigue life in titanium alloys",
        "doi": "10.1063/5.0129037",
        "year": 2023,
        "abstract": "<jats:p>Making reliable predictions of the mechanical behavior of alloys with a prolonged service life is beneficial for many structural applications. In this work, we propose an interpretable machine learning (ML) approach to predict fatigue life cycles (Nf) and creep rupture life (tr) in titanium-based alloys. Chemical compositions, experimental parameters, and alloy processing conditions are employed as descriptors for the development of gradient boost regression models for log-scaled Nf and tr. The models are trained on an extensive experimental dataset, predicting log-scaled Nf and tr with a very small root mean squared error of 0.17 and 0.15, respectively. An intuitive interpretation of the ML models is carried out via SHapley Additive exPlanations (SHAP) to understand the complex interplay of various features with Nf and tr. The SHAP interpretation of the ML models reveals close agreement with the general creep equation and W\u00f6hler curve of fatigue. The approach proposed in this study can accelerate the design of novel Ti-based alloys with desired properties.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Quantifying the impact of uninformative features on the performance of supervised classification and dimensionality reduction algorithms",
        "doi": "10.1063/5.0170229",
        "year": 2023,
        "abstract": "<jats:p>Machine learning approaches have become critical tools in data mining and knowledge discovery, especially when attempting to uncover relationships in high-dimensional data. However, researchers have noticed that a large fraction of features in high-dimensional datasets are commonly uninformative (too noisy or irrelevant). Because optimal feature selection is an NP-hard task, it is essential to understand how uninformative features impact the performance of machine learning algorithms. Here, we conduct systematic experiments on algorithms from a wide range of taxonomy families using synthetic datasets with different numbers of uninformative features and different numbers of patterns to be learned. Upon visual inspection, we classify these algorithms into four groups with varying robustness against uninformative features. For the algorithms in three of the groups, we find that when the number of uninformative features exceeds the number of data instances per pattern to be learned, the algorithms fail to learn the patterns. Finally, we investigate whether increasing the distinguishability of patterns or adding training instances can mitigate the effect of uninformative features. Surprisingly, we find that uninformative features still cause algorithms to suffer big losses in performance, even when patterns should be easily distinguishable. Analyses of real-world data show that our conclusions hold beyond the synthetic datasets we study systematically.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "A tutorial on the Bayesian statistical approach to inverse problems",
        "doi": "10.1063/5.0154773",
        "year": 2023,
        "abstract": "<jats:p>Inverse problems are ubiquitous in science and engineering. Two categories of inverse problems concerning a physical system are (1) estimate parameters in a model of the system from observed input\u2013output pairs and (2) given a model of the system, reconstruct the input to it that caused some observed output. Applied inverse problems are challenging because a solution may (i) not exist, (ii) not be unique, or (iii) be sensitive to measurement noise contaminating the data. Bayesian statistical inversion (BSI) is an approach to tackle ill-posed and/or ill-conditioned inverse problems. Advantageously, BSI provides a \u201csolution\u201d that (i) quantifies uncertainty by assigning a probability to each possible value of the unknown parameter/input and (ii) incorporates prior information and beliefs about the parameter/input. Herein, we provide a tutorial of BSI for inverse problems by way of illustrative examples dealing with heat transfer from ambient air to a cold lime fruit. First, we use BSI to infer a parameter in a dynamic model of the lime temperature from measurements of the lime temperature over time. Second, we use BSI to reconstruct the initial condition of the lime from a measurement of its temperature later in time. We demonstrate the incorporation of prior information, visualize the posterior distributions of the parameter/initial condition, and show posterior samples of lime temperature trajectories from the model. Our Tutorial aims to reach a wide range of scientists and engineers.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Spontaneous muscle activity classification with delay-based reservoir computing",
        "doi": "10.1063/5.0160927",
        "year": 2023,
        "abstract": "<jats:p>Neuromuscular disorders (NMDs) affect various parts of a motor unit, such as the motor neuron, neuromuscular junction, and muscle fibers. Abnormal spontaneous activity (SA) is detected with electromyography (EMG) as an essential hallmark in diagnosing NMD, which causes fatigue, pain, and muscle weakness. Monitoring the effects of NMD calls for new smart devices to collect and classify EMG. Delay-based Reservoir Computing (DRC) is a neuromorphic algorithm with high efficiency in classifying sequential data. This work proposes a new DRC-based algorithm that provides a reference for medical education and training and a second opinion to clinicians to verify NMD diagnoses by detecting SA in muscles. With a sampling frequency of Fs = 64\u00a0kHz, we have classified SA with EMG signals of 1\u00a0s of muscle recordings. Furthermore, the DRC model of size N = 600 nodes has successfully detected SA signals against normal muscle activity with an accuracy of up to 90.7%. The potential of using neuromorphic processing approaches in point-of-care diagnostics, alongside the supervision of a clinician, provides a more comprehensive and reliable clinical profile. Our developed model benefits from the potential to be implemented in physical hardware to provide near-sensor edge computing.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Unsupervised machine learning to analyze corneal tissue surfaces",
        "doi": "10.1063/5.0159502",
        "year": 2023,
        "abstract": "<jats:p>Identifying/classifying damage features on soft materials, such as tissues, is much more challenging than on classical, hard materials\u2014but nevertheless important, especially in the field of bio-tribology. For instance, cartilage samples from osteoarthritic patients exhibit surface damage even at early stages of tissue degeneration, and corneal tissues can be damaged by contact lenses when the ocular lubrication system fails. Here, we employ unsupervised machine learning (ML) methods to assess the surface condition of a soft tissue by detecting and classifying different wear morphologies as well as the severity of surface damage they represent. We show that different clustering methods, especially a k-means clustering algorithm, can indeed achieve a\u2014from a material science point of view\u2014meaningful classification of those tissue samples. Our study pinpoints the ability of unsupervised ML models to guide or even replace human decision processes for the analysis of complex surfaces and topographical datasets that\u2014either owing to their complexity or the sample size\u2014exceed the capability of the human brain.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Benchmarking energy consumption and latency for neuromorphic computing in condensed matter and particle physics",
        "doi": "10.1063/5.0116699",
        "year": 2023,
        "abstract": "<jats:p>The massive use of artificial neural networks (ANNs), increasingly popular in many areas of scientific computing, rapidly increases the energy consumption of modern high-performance computing systems. An appealing and possibly more sustainable alternative is provided by novel neuromorphic paradigms, which directly implement ANNs in hardware. However, little is known about the actual benefits of running ANNs on neuromorphic hardware for use cases in scientific computing. Here, we present a methodology for measuring the energy cost and compute time for inference tasks with ANNs on conventional hardware. In addition, we have designed an architecture for these tasks and estimate the same metrics based on a state-of-the-art analog in-memory computing (AIMC) platform, one of the key paradigms in neuromorphic computing. Both methodologies are compared for a use case in quantum many-body physics in two-dimensional condensed matter systems and for anomaly detection at 40\u00a0MHz rates at the Large Hadron Collider in particle physics. We find that AIMC can achieve up to one order of magnitude shorter computation times than conventional hardware at an energy cost that is up to three orders of magnitude smaller. This suggests great potential for faster and more sustainable scientific computing with neuromorphic hardware.</jats:p>",
        "is_referenced_by_count": 3
    },
    {
        "title": "Physics-constrained 3D convolutional neural networks for electrodynamics",
        "doi": "10.1063/5.0132433",
        "year": 2023,
        "abstract": "<jats:p>We present a physics-constrained neural network (PCNN) approach to solving Maxwell\u2019s equations for the electromagnetic fields of intense relativistic charged particle beams. We create a 3D convolutional PCNN to map time-varying current and charge densities J(r, t) and \u03c1(r, t) to vector and scalar potentials A(r, t) and \u03c6(r, t) from which we generate electromagnetic fields according to Maxwell\u2019s equations: B = \u2207 \u00d7 A and E = \u2212\u2207\u03c6 \u2212 \u2202A/\u2202t. Our PCNNs satisfy hard constraints, such as \u2207\u2009\u00b7\u2009B = 0, by construction. Soft constraints push A and \u03c6 toward satisfying the Lorenz gauge.</jats:p>",
        "is_referenced_by_count": 4
    },
    {
        "title": "Visual explanations of machine learning model estimating charge states in quantum dots",
        "doi": "10.1063/5.0193621",
        "year": 2024,
        "abstract": "<jats:p>Charge state recognition in quantum dot devices is important in the preparation of quantum bits for quantum information processing. Toward auto-tuning of larger-scale quantum devices, automatic charge state recognition by machine learning has been demonstrated. For further development of this technology, an understanding of the operation of the machine learning model, which is usually a black box, will be useful. In this study, we analyze the explainability of the machine learning model estimating charge states in quantum dots by gradient weighted class activation mapping. This technique highlights the important regions in the image for predicting the class. The model predicts the state based on the change transition lines, indicating that human-like recognition is realized. We also demonstrate improvements of the model by utilizing feedback from the mapping results. Due to the simplicity of our simulation and pre-processing methods, our approach offers scalability without significant additional simulation costs, demonstrating its suitability for future quantum dot system expansions.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Digitizing images of electrical-circuit schematics",
        "doi": "10.1063/5.0177755",
        "year": 2024,
        "abstract": "<jats:p>Electrical-circuit schematics are a foundational tool in electrical engineering. A method that can automatically digitalize them is desirable since a knowledge base of such schematics could preserve their functional information as well as provide a database that one can mine to predict more operationally efficient electrical circuits using data analytics and machine learning. We present a workflow that contains a novel pattern-recognition methodology and a custom-trained Optical Character Recognition (OCR) model that can digitalize images of electrical-circuit schematics with minimal configuration. The pattern-recognition and OCR stages of the workflow yield 86.4% and 99.6% success rates, respectively. We also present an extendable option toward predictive circuit-design efficiencies, subject to a large database of images being available. Thereby, data gathered from our pattern-recognition workflow are used to draw network graphs, which are in turn employed to form matrix equations that contain the voltages and currents for all nodes in the circuit in terms of component values. These equations could be applied to a database of electrical-circuit schematics to predict new circuit designs or circuit modifications that offer greater operational efficiency. Alternatively, these network graphs could be converted into simulation programs with integrated circuit emphasis netlists to afford more accurate and computationally automated simulations.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Machine learning for rapid discovery of laminar flow channel wall modifications that enhance heat transfer",
        "doi": "10.1063/5.0187783",
        "year": 2024,
        "abstract": "<jats:p>Numerical simulation of fluid flow plays an essential role in modeling many physical phenomena, which enables technological advancements, contributes to sustainable practices, and expands our understanding of various natural and engineered systems. The calculation of heat transfer in fluid flow in simple flat channels is a relatively easy task for various simulation methods. However, once the channel geometry becomes more complex, numerical simulations become a bottleneck in optimizing wall geometries. We present a combination of accurate numerical simulations of arbitrary, flat, and non-flat channels as well as machine learning models trained on simulated data to predict the drag coefficient and Stanton number. We show that convolutional neural networks (CNNs) can accurately predict target properties at a fraction of the computational cost of numerical simulations. We use CNN models in a virtual high-throughput screening approach to explore a large number of possible, randomly generated wall architectures. Data augmentation techniques are incorporated to enforce physical invariances toward shifting and flipping, contributing to precise prediction for fluid flow and heat transfer characteristics. Moreover, we approach the interpretation of the trained model to better understand relevant channel structures and their influence on heat transfer. The general approach is not only applicable to simple flow setups as presented here but can be extended to more complex tasks, such as multiphase or even reactive unit operations in chemical engineering.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "M3ICRO: Machine learning-enabled compact photonic tensor core based on programmable multi-operand multimode interference",
        "doi": "10.1063/5.0170965",
        "year": 2024,
        "abstract": "<jats:p>Photonic computing shows promise for transformative advancements in machine learning (ML) acceleration, offering ultrafast speed, massive parallelism, and high energy efficiency. However, current photonic tensor core (PTC) designs based on standard optical components hinder scalability and compute density due to their large spatial footprint. To address this, we propose an ultracompact PTC using customized programmable multi-operand multimode interference (MOMMI) devices, named M3ICRO. The programmable MOMMI leverages the intrinsic light propagation principle, providing a single-device programmable matrix unit beyond the conventional computing paradigm of one multiply-accumulate operation per device. To overcome the optimization difficulty of customized devices that often requires time-consuming simulation, we apply ML for optics to predict the device behavior and enable differentiable optimization flow. We thoroughly investigate the reconfigurability and matrix expressivity of our customized PTC and introduce a novel block unfolding method to fully exploit the computing capabilities of a complex-valued PTC for near-universal real-valued linear transformations. Extensive evaluations demonstrate that M3ICRO achieves a 3.5\u20138.9\u00d7 smaller footprint, 1.6\u20134.4\u00d7 higher speed, 9.9\u201338.5\u00d7 higher compute density, 3.7\u201312\u00d7 higher system throughput, and superior noise robustness compared to state-of-the-art coherent PTC designs. It also outperforms electronic digital A100 graphics processing unit by 34.8\u2013403\u00d7 higher throughput while maintaining close-to-digital task accuracy across various ML benchmarks.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "In-memory computing with emerging memory devices: Status and outlook",
        "doi": "10.1063/5.0136403",
        "year": 2023,
        "abstract": "<jats:p>In-memory computing (IMC) has emerged as a new computing paradigm able to alleviate or suppress the memory bottleneck, which is the major concern for energy efficiency and latency in modern digital computing. While the IMC concept is simple and promising, the details of its implementation cover a broad range of problems and solutions, including various memory technologies, circuit topologies, and programming/processing algorithms. This Perspective aims at providing an orientation map across the wide topic of IMC. First, the memory technologies will be presented, including both conventional complementary metal-oxide-semiconductor-based and emerging resistive/memristive devices. Then, circuit architectures will be considered, describing their aim and application. Circuits include both popular crosspoint arrays and other more advanced structures, such as closed-loop memory arrays and ternary content-addressable memory. The same circuit might serve completely different applications, e.g., a crosspoint array can be used for accelerating matrix-vector multiplication for forward propagation in a neural network and outer product for backpropagation training. The different algorithms and memory properties to enable such diversification of circuit functions will be discussed. Finally, the main challenges and opportunities for IMC will be presented.</jats:p>",
        "is_referenced_by_count": 18
    },
    {
        "title": "Sparse subnetwork inference for neural network epistemic uncertainty estimation with improved Hessian approximation",
        "doi": "10.1063/5.0193951",
        "year": 2024,
        "abstract": "<jats:p>Despite significant advances in deep neural networks across diverse domains, challenges persist in safety-critical contexts, including domain shift sensitivity and unreliable uncertainty estimation. To address these issues, this study investigates Bayesian learning for uncertainty handling in modern neural networks. However, the high-dimensional, non-convex nature of the posterior distribution poses practical limitations for epistemic uncertainty estimation. The Laplace approximation, as a cost-efficient Bayesian method, offers a practical solution by approximating the posterior as a multivariate normal distribution but faces computational bottlenecks in precise covariance matrix computation and storage. This research employs subnetwork inference, utilizing only a subset of the parameter space for Bayesian inference. In addition, a Kronecker-factored and low-rank representation is explored to reduce space complexity and computational costs. Several corrections are introduced to converge the approximated curvature to the exact Hessian matrix. Numerical results demonstrate the effectiveness and competitiveness of this method, whereas qualitative experiments highlight the impact of Hessian approximation granularity and parameter space utilization in Bayesian inference on mitigating overconfidence in predictions and obtaining high-quality uncertainty estimates.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Physics-agnostic inverse design using transfer matrices",
        "doi": "10.1063/5.0179457",
        "year": 2024,
        "abstract": "<jats:p>Inverse design is an application of machine learning to device design, giving the computer maximal latitude in generating novel structures, learning from their performance, and optimizing them to suit the designer\u2019s needs. Gradient-based optimizers, augmented by the adjoint method to efficiently compute the gradient, are particularly attractive for this approach and have proven highly successful with finite-element and finite-difference physics simulators. Here, we extend adjoint optimization to the transfer matrix method, an accurate and efficient simulator for a wide variety of quasi-1D physical phenomena. We leverage this versatility to develop a physics-agnostic inverse design framework and apply it to three distinct problems, each presenting a substantial challenge for conventional design methods: optics, designing a multivariate optical element for compressive sensing; acoustics, designing a high-performance anti-sonar submarine coating; and quantum mechanics, designing a tunable double-bandpass electron energy filter.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Imaging in double-casing wells with convolutional neural network based on inception module",
        "doi": "10.1063/5.0191452",
        "year": 2024,
        "abstract": "<jats:p>The evaluation of well integrity in double-casing wells is critical for ensuring well stability, preventing oil and gas leaks, avoiding pollution, and ensuring safety throughout well development and production. However, the current predominant method of assessing cementing quality primarily focuses on single-casing wells, with limited work conducted on double-casing wells. This study introduces a novel approach for evaluating the cementing quality using the Inception module of convolutional neural networks. First, the finite-difference method is employed to generate borehole sonic data corresponding to a variety of model configurations, which are used to train a neural network that learns spatial features from the borehole sonic data to reconstruct the slowness model. By adjusting the network architecture and parameters, it is discovered that a neural network with two blocks and 4096 nodes in the fully connected layer demonstrated the best imaging results and exhibited strong anti-noise capabilities. The proposed method is validated using practical wellbore size models, demonstrating excellent results and offering a more effective means of evaluating wellbore integrity in double-casing wells. In addition, dipole acoustic logging data are used to conduct slowness model imaging of the compressional (P-) wave and shear (S-) wave in double-casing wells to verify the feasibility of cementing quality evaluation. The developed method contributes to more accurate evaluations of wellbore integrity for the oil and gas industry, leading to improved safety and environmental outcomes.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Artificial neural network-based streamline tracing strategy applied to hypersonic waverider design",
        "doi": "10.1063/5.0127034",
        "year": 2023,
        "abstract": "<jats:p>Streamline tracing in hypersonic flows is essential for designing a high-performance waverider and intake. Conventionally, the streamline equations are solved after obtaining the velocity field over a basic flow field from simplified flow differential equations or three-dimensional computational fluid dynamics. The hypersonic waverider shape is generated by repeatedly applying the streamline tracing approach along several planes. This approach is computationally expensive for iterative waverider optimization. We provide a novel strategy where an Artificial Neural Network (ANN) is trained to directly predict the streamlines without solving the differential equations. We consider the standard simple cone-derived waverider using Taylor\u2013Maccoll equations for the conical flow field as a template for the study. First, the streamlines from the shock are solved for a wide range of cone angle and Mach number conditions resulting in an extensive database. The streamlines are parameterized by a third-order polynomial, and an ANN is trained to predict the coefficients of the polynomial for arbitrary inputs of Mach number, cone angle, and streamline originating locations. We apply this strategy to design a cone-derived waverider and compare the geometry obtained with the standard conical waverider design method and the simplified waverider design method. The ANN technique is highly accurate, with a difference of 0.68% from the standard method in the coordinates of the waverider. The performance of the three waveriders is compared using Reynolds averaged Navier\u2013Stokes simulations. The ANN-derived waverider does not indicate severe flow spillage at the leading edge. The new ANN-based approach is 20 times faster than the standard method.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Designing composition ratio of magnetic alloy multilayer for transverse thermoelectric conversion by Bayesian optimization",
        "doi": "10.1063/5.0140332",
        "year": 2023,
        "abstract": "<jats:p>We demonstrated the effectiveness of the machine learning method combined with first-principles calculations for the enhancement of the anomalous Nernst effect (ANE) of multilayers. The composition ratio of CoNi homogeneous alloy superlattices was optimized by Bayesian optimization so as to maximize the transverse thermoelectric conductivity (\u03b1xy). The nonintuitive optimal composition with a large \u03b1xy of \u223c10\u00a0A\u00a0K\u22121\u00a0m\u22121 was identified through the two-step Bayesian optimization using rough and fine candidate pools. The Berry curvature and band dispersion analyses revealed that \u03b1xy is enhanced by the appearance of the flat band near the Fermi level due to the multilayer formation. The magnitude of the energy derivative of the anomalous Hall conductivity increases owing to the large Berry curvature near the flat band along the R-M high symmetry line, which emerges only in the optimized superlattice, leading to the \u03b1xy enhancement. The effective method verified here will broaden the choices of ANE materials to more complex systems and, therefore, lead to the development of transverse thermoelectric conversion technologies.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Learning the stable and metastable phase diagram to accelerate the discovery of metastable phases of boron",
        "doi": "10.1063/5.0175994",
        "year": 2024,
        "abstract": "<jats:p>Boron, an element of captivating chemical intricacy, has been surrounded by controversies ever since its discovery in 1808. The complexities of boron stem from its unique position between metals and insulators in the Periodic Table. Recent computational studies have shed light on some of the stable boron allotropes. However, the demand for multifunctionality necessitates the need to go beyond the stable phases into the realm of metastability and explore the potentially vast but elusive metastable phases of boron. Traditional search for stable phases of materials has focused on identifying materials with the lowest enthalpy. Here, we introduce a workflow that uses reinforcement learning coupled with decision trees, such as Monte Carlo tree search, to search for stable and metastable boron phases, with enthalpy as the objective. We discover new boron metastable phases and construct a phase diagram that locates their phase space (T, P) at different levels of metastability (\u0394G) from the ground state and provides useful information on the domains of relative stability of the various stable and metastable boron phases.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Simulation of the effect of material properties on yttrium oxide memristor-based artificial neural networks",
        "doi": "10.1063/5.0143926",
        "year": 2023,
        "abstract": "<jats:p>This paper reports a simulation study concerning the effect of yttrium oxide stoichiometry on output features of a memristor-based single layer perceptron neural network. To carry out this investigation, a material-oriented behavioral compact model for bipolar-type memristive devices was developed and tested. The model is written for the SPICE (Simulation Program with Integrated Circuits Emphasis) simulator and considers as one of its inputs a measure of the oxygen flow used during the deposition of the switching layer. After a thorough statistical calibration of the model parameters using experimental current\u2013voltage characteristics associated with different fabrication conditions, the corresponding curves were simulated and the results were compared with the original data. In this way, the average switching behavior of the structures (low and high current states, set and reset voltages, etc.) as a function of the oxygen content can be forecasted. In a subsequent phase, the collective response of the devices when used in a neural network was investigated in terms of the output features of the network (mainly power dissipation and power efficiency). The role played by parasitic elements, such as the line resistance and the read voltage influence on the inference accuracy, was also explored. Since a similar strategy can be applied to any other material-related fabrication parameter, the proposed approach opens up a new dimension for circuit designers, as the behavior of complex circuits employing devices with specific characteristics can be realistically assessed before fabrication.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Calibration in machine learning uncertainty quantification: Beyond consistency to target adaptivity",
        "doi": "10.1063/5.0174943",
        "year": 2023,
        "abstract": "<jats:p>Reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks is becoming the focus of many studies in materials and chemical science. It is now well understood that average calibration is insufficient, and most studies implement additional methods for testing the conditional calibration with respect to uncertainty, i.e., consistency. Consistency is assessed mostly by so-called reliability diagrams. There exists, however, another way beyond average calibration, which is conditional calibration with respect to input features, i.e., adaptivity. In practice, adaptivity is the main concern of the final users of the ML-UQ method, seeking the reliability of predictions and uncertainties for any point in the feature space. This article aims to show that consistency and adaptivity are complementary validation targets and that good consistency does not imply good adaptivity. An integrated validation framework is proposed and illustrated with a representative example.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Seizure detection using dynamic memristor-based reservoir computing and leaky integrate-and-fire neuron for post-processing",
        "doi": "10.1063/5.0171274",
        "year": 2023,
        "abstract": "<jats:p>Epilepsy is a prevalent neurological disorder, rendering the development of automated seizure detection systems imperative. While complex machine learning models are powerful, their training and hardware deployment remain challenging. The reservoir computing system offers a low-cost solution in terms of both hardware requirements and training. In this paper, we introduce a compact reservoir computing system for seizure detection, based on the \u03b1-In2Se3 dynamic memristors. Leaky integrate-and-fire neurons are used for post-processing the output of the system, and experimental results indicate their effectiveness in suppressing erroneous outputs, where both accuracy and specificity are enhanced by over 2.5%. The optimized compact reservoir system achieves 96.40% accuracy, 86.34% sensitivity, and 96.56% specificity in seizure detection tasks. This work demonstrates the feasibility of using reservoir computing for seizure detection and shows its potential for future application in extreme edge devices.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Advancing magnetic material discovery through machine learning: Unveiling new manganese-based materials",
        "doi": "10.1063/5.0171320",
        "year": 2023,
        "abstract": "<jats:p>Magnetic materials are used in a variety of applications, such as electric generators, speakers, hard drives, MRI machines, etc. Discovery of new magnetic materials with desirable properties is essential for advancement in these applications. In this research article, we describe the development and validation of a machine-learning model to discover new manganese-based stable magnetic materials. The machine learning model is trained on the input data from the Materials Project database to predict the magnetization and formation energy of the materials. New hypothetical structures are made using the substitution method, and the properties are predicted using the machine learning model to select the materials with desired properties. Harnessing the power of machine learning allows us to intelligently narrow down the vast pool of potential candidates. By doing so, we deftly reduce the number of materials that warrant in-depth examination using density functional theory, rendering the task more manageable and efficient. The selected materials, seemingly promising with their magnetic potential, undergo a meticulous validation process using the Vienna Ab initio Simulation Package, grounded in density functional theory. Our results underscore the paramount significance of input data in the efficacy of the machine learning model. Particularly in the realm of magnetic materials, the proper initialization of atomic magnetic spins holds the key to converging upon the true magnetic state of each material.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Using the IBM analog in-memory hardware acceleration kit for neural network training and inference",
        "doi": "10.1063/5.0168089",
        "year": 2023,
        "abstract": "<jats:p>Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training. However, the noisy and non-linear device characteristics and the non-ideal peripheral circuitry in AIMC chips require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing. In this Tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit. AIHWKit is a Python library that simulates inference and training of DNNs using AIMC. We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training. We also present an overview of the Analog AI Cloud Composer, a platform that provides the benefits of using the AIHWKit simulation in a fully managed cloud setting along with physical AIMC hardware access, freely available at https://aihw-composer.draco.res.ibm.com. Finally, we show examples of how users can expand and customize AIHWKit for their own needs. This Tutorial is accompanied by comprehensive Jupyter Notebook code examples that can be run using AIHWKit, which can be downloaded from https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial.</jats:p>",
        "is_referenced_by_count": 3
    },
    {
        "title": "Measuring thermal profiles in high explosives using neural networks",
        "doi": "10.1063/5.0183886",
        "year": 2024,
        "abstract": "<jats:p>We present a new method for calculating the temperature profile of high explosive (HE) material using a Convolutional Neural Network (CNN). To train/test the CNN, we have developed a hybrid experiment/simulation method for collecting acoustic and temperature data. We experimentally heat cylindrical containers of HE material until detonation/deflagration, where we continuously measure the acoustic bursts through the HE using multiple acoustic transducers lined around the exterior container circumference. However, measuring the temperature profile in the HE in an experiment would require inserting a large number of thermal probes, which would disrupt the heating process. Thus, we use two thermal probes, one at the HE center and one at the wall. We then use numerical simulation of the heating process to calculate the temperature distribution and correct the simulated temperatures based on the experimental center and wall temperatures. We calculate temperature errors on the order of 15\u2009\u00b0C, which is \u223c12% of the range of temperatures in the experiment. We also investigate how the algorithm\u2019s accuracy is affected by the number of acoustic receivers used to collect each measurement and the resolution of the temperature prediction. This work provides a means of assessing the safety status of HE material, which cannot be achieved using existing temperature measurement methods. In addition, it has implications for a range of other applications where internal temperature profile measurements would provide critical information. These applications include detecting chemical reactions, observing thermodynamic processes such as combustion, monitoring metal or plastic casting, determining the energy density in thermal storage capsules, and identifying abnormal battery operations.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "LiNLNet: Gauging required nonlinearity in deep neural networks",
        "doi": "10.1063/5.0134713",
        "year": 2023,
        "abstract": "<jats:p>Feedforward deep neural networks (DNNs) commonly involve layer-wise linear operations and subsequent nonlinear operations, which are repeated through all layers. The nonlinear operations by nonlinear activations in each layer remarkably enhance the expressiveness of DNNs, resulting in the great success in a variety of application domains. Although the necessity of layer-wise nonlinear operations is agreed, the optimal nonlinearity for each layer in a given DNN is not clear. In this regard, we propose an easy-to-use method to layer-wise measure the optimal nonlinearity for a given DNN using its replica termed a linear-nonlinear network (LiNLNet). The key to the LiNLNet is the use of linear-nonlinear units (LiNLUs) whose degree of nonlinearity is parameterized by a trainable parameter p. The parameter p is shared among all LiNLUs in a given layer, thus indicating the layer-wise optimal nonlinearity. This method allows layer-level pruning such that the layers that do not require nonlinearity are merged into the subsequent layers, reducing computational complexity. For proofs of concept, we applied the proposed method to a MLP, AlexNet, VGG16, and ResNet18 on CIFAR-10 and ImageNet. The results commonly indicate the last hidden layer as a linear layer that may be merged into the output layer, reducing memory usage by 27% while maintaining the accuracy for LiNL-AlexNet on ImageNet.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "An efficiency-driven, correlation-based feature elimination strategy for small datasets",
        "doi": "10.1063/5.0118207",
        "year": 2023,
        "abstract": "<jats:p>With big datasets and highly efficient algorithms becoming increasingly available for many problem sets, rapid advancements and recent breakthroughs achieved in the field of machine learning encourage more and more scientific fields to make use of such a computational data analysis. Still, for many research problems, the amount of data available for training a machine learning (ML) model is very limited. An important strategy to combat the problems arising from data sparsity is feature elimination\u2014a method that aims at reducing the dimensionality of an input feature space. Most such strategies exclusively focus on analyzing pairwise correlations, or they eliminate features based on their relation to a selected output label or by optimizing performance measures of a certain ML model. However, those strategies do not necessarily remove redundant information from datasets and cannot be applied to certain situations, e.g., to unsupervised learning models. Neither of these limitations applies to the network-based, correlation-driven redundancy elimination (NETCORE) algorithm introduced here, where the size of a feature vector is reduced by considering both redundancy and elimination efficiency. The NETCORE algorithm is model-independent, does not require an output label, and is applicable to all kinds of correlation topographies within a dataset. Thus, this algorithm has the potential to be a highly beneficial preprocessing tool for various machine learning pipelines.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Multi-objective generative design of three-dimensional material structures",
        "doi": "10.1063/5.0169432",
        "year": 2023,
        "abstract": "<jats:p>Generative design for materials has recently gained significant attention due to the rapid evolution of generative deep learning models. There have been a few successful generative design demonstrations of molecular-level structures with the help of graph neural networks. However, in the realm of macroscale material structures, most of the works are targeting two-dimensional, ungoverned structure generations. Hindered by the complexity of 3D structures, it is hard to extract customized structures with multiple desired properties from a large, unexplored design space. Here we report a novel framework, a multi-objective driven Wasserstein generative adversarial network (WGAN), to implement inverse designs of 3D structures according to given geometrical, structural, and mechanical requirements. Our framework consists of a WGAN-based network that generates 3D structures possessing geometrical and structural features learned from the target dataset. Besides, multiple objectives are introduced to our framework for the control of mechanical property and isotropy of the structures. An accurate surrogate model is incorporated into the framework to perform efficient prediction on the properties of generated structures in training iterations. With multiple objectives combined by their weight and the 3D WGAN acting as a soft constraint to regulate features that are hard to define by the traditional method, our framework has proven to be capable of tuning the properties of the generated structures in multiple aspects while keeping the selected structural features. The feasibility of a small dataset and the scalability of the objectives of other properties make our work an effective approach to provide fast and automated structure designs for various functional materials.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Automatic graph representation algorithm for heterogeneous catalysis",
        "doi": "10.1063/5.0140487",
        "year": 2023,
        "abstract": "<jats:p>One of the most appealing aspects of machine learning for material design is its high throughput exploration of chemical spaces, but to reach the ceiling of machine learning-aided exploration, more than current model architectures and processing algorithms are required. New architectures such as graph neural networks have seen significant research investments recently. For heterogeneous catalysis, defining substrate intramolecular bonds and adsorbate/substrate intermolecular bonds is a time-consuming and challenging process. Before applying a model, dataset pre-processing, node/bond descriptor design, and specific model constraints have to be considered. In this work, a framework designed to solve these issues is presented in the form of an automatic graph representation algorithm (AGRA) tool to extract the local chemical environment of metallic surface adsorption sites. This tool is able to gather multiple adsorption geometry datasets composed of different systems and combine them into a single model. To show AGRA\u2019s excellent transferability and reduced computational cost compared to other graph representation methods, it was applied to five different catalytic reaction datasets and benchmarked against the Open Catalyst Projects graph representation method. The two oxygen reduction reaction (ORR) datasets with O/OH adsorbates obtained 0.053\u00a0eV root-mean-square deviation (RMSD) when combined together, whereas the three carbon dioxide reduction reaction datasets with CHO/CO/COOH obtained an average performance of 0.088\u00a0eV RMSD. To further display the algorithm\u2019s versatility and extrapolation ability, a model was trained on a subset combination of all five datasets with an RMSD of 0.105\u00a0eV. This universal model was then used to predict a wide range of adsorption energies and an entirely new ORR catalyst system, which was then verified through density functional theory calculations.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Analysis of Brownian motion trajectories of non-spherical nanoparticles using deep learning",
        "doi": "10.1063/5.0160979",
        "year": 2023,
        "abstract": "<jats:p>As nanoparticles are being put to practical use as useful materials in the medical, pharmaceutical, and industrial fields, the importance of technologies that can evaluate not only nanoparticle populations of homogeneous size and density but also those of rich diversity is increasing. Nano-tracking analysis (NTA) has been commercialized and widely used as a method to measure individual nanoparticles in liquids and evaluate their size distribution by analyzing Brownian motion. We have combined deep learning (DL) for NTA to extract more property information and explored a methodology to achieve an evaluation for individual particles to understand their diversity. Practical NTA always assumes spherical shape when quantifying particle size using the Stokes\u2013Einstein equation, but it is not possible to verify whether the measured particles are truly spherical. We developed a DL model that predicts the shape of nanoparticles using time series trajectory data of BM obtained from NTA measurements to address this problem. As a result, we were able to discriminate with \u223c80% accuracy between spherical and rod-shaped gold nanoparticles of different shapes, which are evaluated to have nearly equal particle size without any discrimination by conventional NTA. Furthermore, we demonstrated that the mixing ratio of spherical and rod-shaped nanoparticles can be quantitatively estimated from measured data of mixed samples of nanoparticles. This result suggests that it is possible to evaluate particle shape by applying DL analysis to NTA measurements, which was previously considered impossible, and opens the way to further value-added NTA.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Data-driven design for enhanced efficiency of Sn-based perovskite solar cells using machine learning",
        "doi": "10.1063/5.0177271",
        "year": 2023,
        "abstract": "<jats:p>In this study, a novel three-step learning-based machine learning (ML) methodology is developed utilizing 26\u2009000 experimental records from The Perovskite Database Project. A comprehensive set of 29 features encompassing both categorical and numerical data was utilized to train various ML models for various solar cell performance metrics, including open-circuit voltage (VOC), short-circuit current (JSC), fill factor (FF), and power conversion efficiency (PCE). The model accuracy was assessed using four key metrics: mean absolute error, mean square error, root mean square error, and R2 score. Among the constructed models, random forest (RF) emerged as the standout performer, boasting an R2 score of 0.70 for PCE. This RF model was then used for prediction on the large, optimized design pool of Sn-based perovskite data with intent to probe a viable non-toxic substitute to the standard Pb-based absorber. A three-step algorithm was tailored, which led to the discovery of a new set of feature combinations, showcasing a PCE improvement over the existing peak performance of Sn-based devices. The key aspects identified were device architecture, dimensionality, and deposition procedures for essential layers, including the electron transport layer, the hole transport layer, the perovskite absorber layer, and the back-contact. Through consideration of these features, an impressive increase in PCE was achieved. There was a 28.35% increase in PCE from 12.24% to 15.71% for architecture optimization and a 24.6% increase in PCE from 12.24% to 15.25% for deposition method optimization. This study additionally addresses the effective implementation of target encoding applied to a diverse set of categorical feature labels. The data-driven methodology proposed in this study allows scientists to efficiently identify an optimal architecture and deposition parameters for non-toxic Sn-based perovskite materials with a much higher anticipated device PCE compared to traditional trial-and-error analyses. Further exploration and exploitation of the current investigation is expected to lead to successful and sustainable development of highly efficient Sn-based perovskite solar cells.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Improving the mechanical properties of Cantor-like alloys with Bayesian optimization",
        "doi": "10.1063/5.0179844",
        "year": 2024,
        "abstract": "<jats:p>The search for better compositions in high entropy alloys is a formidable challenge in materials science. Here, we demonstrate a systematic Bayesian optimization method to enhance the mechanical properties of the paradigmatic five-element Cantor alloy in silico. This method utilizes an automated loop with an online database, a Bayesian optimization algorithm, thermodynamic modeling, and molecular dynamics simulations. Starting from the equiatomic Cantor composition, our approach optimizes the relative fractions of its constituent elements, searching for better compositions while maintaining the thermodynamic phase stability. With 24 steps, we find Fe21Cr20Mn5Co20Ni34 with a yield stress improvement of 58%, and with 72 steps, we find Fe6Cr22Mn5Co32Ni35 where the yield stress has improved by 74%. These optimized compositions correspond to Ni-rich medium entropy alloys with enhanced mechanical properties and superior face-centered-cubic phase stability compared to the traditional equiatomic Cantor alloy. The automatic approach devised here paves the way for designing high entropy alloys with tailored properties, opening avenues for numerous potential applications.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Predicting wind farm wake losses with deep convolutional hierarchical encoder\u2013decoder neural networks",
        "doi": "10.1063/5.0168973",
        "year": 2024,
        "abstract": "<jats:p>Wind turbine wakes are the most significant factor affecting wind farm performance, decreasing energy production and increasing fatigue loads in downstream turbines. Wind farm turbine layouts are designed to minimize wake interactions using a suite of predictive models, including analytical wake models and computational fluid dynamics simulations (CFD). CFD simulations of wind farms are time-consuming and computationally expensive, which hinder their use in optimization studies that require hundreds of simulations to converge to an optimal turbine layout. In this work, we propose DeepWFLO, a deep convolutional hierarchical encoder\u2013decoder neural network architecture, as an image-to-image surrogate model for predicting the wind velocity field for Wind Farm Layout Optimization (WFLO). We generate a dataset composed of image representations of the turbine layout and undisturbed flow field in the wind farm, as well as images of the corresponding wind velocity field, including wake effects generated with both analytical models and CFD simulations. The proposed DeepWFLO architecture is then trained and optimized through supervised learning with an application-tailored loss function that considers prediction errors in both wind velocity and energy production. Results on a commonly used test case show median velocity errors of 1.0%\u20138.0% for DeepWFLO networks trained with analytical and CFD data, respectively. We also propose a model-fusion strategy that uses analytical wake models to generate an additional input channel for the network, resulting in median velocity errors below 1.8%. Spearman rank correlations between predictions and data, which evidence the suitability of DeepWFLO for optimization purposes, range between 92.3% and 99.9%.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Autonomous convergence of STM control parameters using Bayesian optimization",
        "doi": "10.1063/5.0185362",
        "year": 2024,
        "abstract": "<jats:p>Scanning tunneling microscopy (STM) is a widely used tool for atomic imaging of novel materials and their surface energetics. However, the optimization of the imaging conditions is a tedious process due to the extremely sensitive tip\u2013surface interaction, thus limiting the throughput efficiency. In this paper, we deploy a machine learning (ML)-based framework to achieve optimal atomically resolved imaging conditions in real time. The experimental workflow leverages the Bayesian optimization (BO) method to rapidly improve the image quality, defined by the peak intensity in the Fourier space. The outcome of the BO prediction is incorporated into the microscope controls, i.e., the current setpoint and the tip bias, to dynamically improve the STM scan conditions. We present strategies to either selectively explore or exploit across the parameter space. As a result, suitable policies are developed for autonomous convergence of the control parameters. The ML-based framework serves as a general workflow methodology across a wide range of materials.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Estimation of TbCo composition from local-minimum-energy magnetic images taken by magneto-optical Kerr effect microscope by using machine learning",
        "doi": "10.1063/5.0160970",
        "year": 2023,
        "abstract": "<jats:p>Recently, the incorporation of machine learning (ML) has heralded significant advancements in materials science. For instance, in spintronics, it has been shown that magnetic parameters, such as the Dzyaloshinskii\u2013Moriya interaction, can be estimated from magnetic domain images using ML. Magnetic materials exhibit hysteresis, leading to numerous magnetic states with locally minimized energy (LME) even within a single sample. However, it remains uncertain whether these parameters can be derived from LME states. In our research, we explored the estimation of material parameters from an LME magnetic state using a convolutional neural network. We introduced a technique to manipulate LME magnetic states, combining the ac demagnetizing method with the magneto-optical Kerr effect. By applying this method, we generated multiple LME magnetic states from a single sample and successfully estimated its material composition. Our findings suggest that ML emphasizes not the global domain structures that are readily perceived by humans but the more subtle local domain structures that are often overlooked. Adopting this approach could potentially facilitate the estimation of magnetic parameters from any state observed in experiments, streamlining experimental processes in spintronics.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "On-device edge-learning for cardiac abnormality detection using a bio-inspired and spiking shallow network",
        "doi": "10.1063/5.0191571",
        "year": 2024,
        "abstract": "<jats:p>This work introduces on-device edge learning for cardiac abnormality detection by merging spiking 2D Convolutional Long-Short-Term Memory (ConvLSTM2D) with a bio-inspired shallow neural network, referred to as Closed-form Continuous-time (CfC), to form the sCCfC model. The model achieves an F1 score and AUROC of 0.82 and 0.91 in cardiac abnormalities detection. These results are comparable to the non-spiking ConvLSTM2D\u2013CfC (ConvCfC) model [Huang et\u00a0al., J. Cardiovasc. Transl. Res. (published online, 2024)]. Notably, the sCCfC model demonstrates a significantly higher energy efficiency with an estimated power consumption of 4.68 \u03bcJ/Inf (per inference) on an emulated Loihi\u2019s neuromorphic chip architecture, in contrast to ConvCfC model\u2019s consumption of 450 \u03bcJ/Inf on a conventional processor. In addition, as a proof-of-concept, we deployed the sCCfC model on the conventional and relatively resource-constrained Radxa Zero, which is equipped with an Amlogic S905Y2 processor for on-device training, which resulted in performance improvements. After initial training of two epochs on a conventional Graphics Processing Unit, the F1 score and AUROC improved from 0.46 and 0.65 to 0.56 and 0.73, respectively, with five additional epochs of on-device training. Furthermore, when presented with a new dataset, the sCCfC model showcases strong out-of-sample generalization capabilities that can constitute a pseudo-perspective test, achieving an F1 score and AUROC of 0.71 and 0.86, respectively. The spiking sCCfC also outperforms the non-spiking ConvCfC model in robustness regarding effectively handling missing electrocardiogram (ECG) channels during inference. The model\u2019s efficacy extends to single-lead ECG analysis, demonstrating reasonable accuracy in this context, while the focus of our work has been on the computational and memory complexities of the model.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Scanning probe microscopy in the age of machine learning",
        "doi": "10.1063/5.0160568",
        "year": 2023,
        "abstract": "<jats:p>Scanning probe microscopy (SPM) has revolutionized our ability to explore the nanoscale world, enabling the imaging, manipulation, and characterization of materials at the atomic and molecular level. However, conventional SPM techniques suffer from limitations, such as slow data acquisition, low signal-to-noise ratio, and complex data analysis. In recent years, the field of machine learning (ML) has emerged as a powerful tool for analyzing complex datasets and extracting meaningful patterns and features in multiple fields. The combination of ML with SPM techniques has the potential to overcome many of the limitations of conventional SPM methods and unlock new opportunities for nanoscale research. In this review article, we will provide an overview of the recent developments in ML-based SPM, including its applications in topography imaging, surface characterization, and secondary imaging modes, such as electrical, spectroscopic, and mechanical datasets. We will also discuss the challenges and opportunities of integrating ML with SPM techniques and highlight the potential impact of this interdisciplinary field on various fields of science and engineering.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Self-supervised learning of shedding droplet dynamics during steam condensation",
        "doi": "10.1063/5.0188620",
        "year": 2024,
        "abstract": "<jats:p>Knowledge of condensate shedding droplet dynamics provides important information for the characterization of two-phase heat and mass transfer phenomena. Detecting and segmenting the droplets during shedding requires considerable time and effort if performed manually. Here, we developed a self-supervised deep learning model for segmenting shedding droplets from a variety of dropwise and filmwise condensing surfaces. The model eliminates the need for image annotation by humans in the training step and, therefore, reduces labor significantly. The trained model achieved an average accuracy greater than 0.9 on a new unseen test dataset. After extracting the shedding droplet size and speed, we developed a data-driven model for shedding droplet dynamics based on condensation heat flux and surface properties such as wettability and tube diameter. Our results demonstrate that condensate droplet departure size is both heat flux and tube size dependent and follows different trends based on the condensation mode. The results of this work provide an annotation-free methodology for falling droplet segmentation as well as a statistical understanding of droplet dynamics during condensation.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Glass transition of amorphous polymeric materials informed by machine learning",
        "doi": "10.1063/5.0137357",
        "year": 2023,
        "abstract": "<jats:p>The glass transition temperature (Tg) is used to determine thermophysical properties of polymer materials and is often considered one of the most important descriptors. Methods for predicting various physical properties of materials based on machine learning algorithms and key molecular descriptors are efficient and accurate. However, it still needs improvements because an overly complex model is less practical and difficult to generalize. In addition, obtaining a large number of samples to achieve accurate predictions remains a challenge due to the complex and lengthy experimental process. In this work, based on Tg of 100 polymers, we use a feature selection algorithm combining FeatureWiz and the least absolute shrinkage and selection operator to quickly select molecular descriptors that are minimally redundant and maximally relevant to Tg. The processed dataset is interpolated from the original dataset using the nearest neighbor interpolation algorithm to solve the data deficiency problem. Finally, the synthetic minority oversampling technique algorithm is used to solve the data imbalance problem. The augmented dataset is used to construct the extreme gradient boosting prediction model to achieve good prediction accuracy. The experimental results demonstrate the robustness of the proposed model and the accuracy of its prediction results.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Flexible optoelectronic synaptic transistors for neuromorphic visual systems",
        "doi": "10.1063/5.0163926",
        "year": 2023,
        "abstract": "<jats:p>Neuromorphic visual systems that integrate the functionalities of sensing, memory, and processing are expected to overcome the shortcomings of conventional artificial visual systems, such as data redundancy, data access delay, and high-energy consumption. Neuromorphic visual systems based on emerging flexible optoelectronic synaptic devices have recently opened up innovative applications, such as robot visual perception, visual prosthetics, and artificial intelligence. Various flexible optoelectronic synaptic devices have been fabricated, which are either two-terminal memristors or three-terminal transistors. In flexible optoelectronic synaptic transistors (FOSTs), the synaptic weight can be modulated by the electricity and light synergistically, which endows the neuromorphic visual systems with versatile functionalities. In this Review, we present an overview of the working mechanisms, device structures, and active materials of FOSTs. Their applications in neuromorphic visual systems for color recognition, image recognition and memory, motion detection, and pain perception are presented. Perspectives on the development of FOSTs are finally outlined.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Training self-learning circuits for power-efficient solutions",
        "doi": "10.1063/5.0181382",
        "year": 2024,
        "abstract": "<jats:p>As the size and ubiquity of artificial intelligence and computational machine learning models grow, the energy required to train and use them is rapidly becoming economically and environmentally unsustainable. Recent laboratory prototypes of self-learning electronic circuits, such as \u201cphysical learning machines,\u201d open the door to analog hardware that directly employs physics to learn desired functions from examples at a low energy cost. In this work, we show that this hardware platform allows for an even further reduction in energy consumption by using good initial conditions and a new learning algorithm. Using analytical calculations, simulations, and experiments, we show that a trade-off emerges when learning dynamics attempt to minimize both the error and the power consumption of the solution\u2014greater power reductions can be achieved at the cost of decreasing solution accuracy. Finally, we demonstrate a practical procedure to weigh the relative importance of error and power minimization, improving the power efficiency given a specific tolerance to error.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "3D CNN and grad-CAM based visualization for predicting generation of dislocation clusters in multicrystalline silicon",
        "doi": "10.1063/5.0156044",
        "year": 2023,
        "abstract": "<jats:p>We propose a machine learning-based technique to address the crystallographic characteristics responsible for the generation of crystal defects. A convolutional neural network was trained with pairs of optical images that display the characteristics of the crystal and photoluminescence images that show the distributions of crystal defects. The model was trained to predict the existence of crystal defects at the center pixel of the given image from its optical features. Prediction accuracy and separability were enhanced by feeding three-dimensional data and data augmentation. The prediction was successful with a high area under the curve of over 0.9 in a receiver operating characteristic curve. Likelihood maps showing the distributions of the predicted defects are in good resemblance with the correct distributions. Using the trained model, we visualized the most important regions to the predicted class by gradient-based class activation mapping. The extracted regions were found to contain mostly particular grains where the grain boundaries changed greatly due to crystal growth and clusters of small grains. This technique is beneficial in providing a rapid and statistical analysis of various crystal characteristics because the features of optical images are often complex and difficult to interpret. The interpretations can help us understand the physics of crystal growth and the effects of crystallographic characteristics on the generation of detrimental defects. We believe that this technique will contribute to the development of a better fabrication process for high-performance multicrystalline materials.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Intelligent performance inference: A graph neural network approach to modeling maximum achievable throughput in optical networks",
        "doi": "10.1063/5.0137426",
        "year": 2023,
        "abstract": "<jats:p>One of the key performance metrics for optical networks is the maximum achievable throughput for a given network. Determining it, however, is a nondeterministic polynomial time (NP) hard optimization problem, often solved via computationally expensive integer linear programming (ILP) formulations. These are infeasible to implement as objectives, even on very small node scales of a few tens of nodes. Alternatively, heuristics are used although these, too, require considerable computation time for a large number of networks. There is, thus, a need for an ultra-fast and accurate performance evaluation of optical networks. For the first time, we propose the use of a geometric deep learning model, message passing neural networks (MPNNs), to learn the relationship between node and edge features, the network structure, and the maximum achievable network throughput. We demonstrate that MPNNs can accurately predict the maximum achievable throughput while reducing the computational time by up to five-orders of magnitude compared to the ILP for small networks (10\u201315 nodes) and compared to a heuristic for large networks (25\u2013100 nodes)\u2014proving their suitability for the design and optimization of optical networks on different time- and distance-scales.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Attention hybrid variational net for accelerated MRI reconstruction",
        "doi": "10.1063/5.0165485",
        "year": 2023,
        "abstract": "<jats:p>The application of compressed sensing (CS)-enabled data reconstruction for accelerating magnetic resonance imaging (MRI) remains a challenging problem. This is due to the fact that the information lost in k-space from the acceleration mask makes it difficult to reconstruct an image similar to the quality of a fully sampled image. Multiple deep learning-based structures have been proposed for MRI reconstruction using CS, in both the k-space and image domains, and using unrolled optimization methods. However, the drawback of these structures is that they are not fully utilizing the information from both domains (k-space and image). Herein, we propose a deep learning-based attention hybrid variational network that performs learning in both the k-space and image domains. We evaluate our method on a well-known open-source MRI dataset (652 brain cases and 1172 knee cases) and a clinical MRI dataset of 243 patients diagnosed with strokes from our institution to demonstrate the performance of our network. Our model achieves an overall peak signal-to-noise ratio/structural similarity of 40.92 \u00b1 0.29/0.9577 \u00b1 0.0025 (fourfold) and 37.03 \u00b1 0.25/0.9365 \u00b1 0.0029 (eightfold) for the brain dataset, 31.09 \u00b1 0.25/0.6901 \u00b1 0.0094 (fourfold) and 29.49 \u00b1 0.22/0.6197 \u00b1 0.0106 (eightfold) for the knee dataset, and 36.32 \u00b1 0.16/0.9199 \u00b1 0.0029 (20-fold) and 33.70 \u00b1 0.15/0.8882 \u00b1 0.0035 (30-fold) for the stroke dataset. In addition to quantitative evaluation, we undertook a blinded comparison of image quality across networks performed by a subspecialty trained radiologist. Overall, we demonstrate that our network achieves a superior performance among others under multiple reconstruction tasks.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "A cloud platform for sharing and automated analysis of raw data from high throughput polymer MD simulations",
        "doi": "10.1063/5.0160937",
        "year": 2023,
        "abstract": "<jats:p>Open material databases storing thousands of material structures and their properties have become the cornerstone of modern computational materials science. Yet, the raw simulation outputs are generally not shared due to their huge size. In this work, we describe a cloud-based platform to enable fast post-processing of the trajectories and to facilitate sharing of the raw data. As an initial demonstration, our database includes 6286 molecular dynamics trajectories for amorphous polymer electrolytes (5.7 terabytes of data). We create a public analysis library at https://github.com/TRI-AMDD/htp_md to extract ion transport properties from the raw data using expert-designed functions and machine learning models. The analysis is run automatically on the cloud, and the results are uploaded onto an open database. Our platform encourages users to contribute both new trajectory data and analysis functions via public interfaces. Finally, we create a front-end user interface at https://www.htpmd.matr.io/ for browsing and visualization of our data. We envision the platform to be a new way of sharing raw data and new insights for the materials science community.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "KoopmanLab: Machine learning for solving complex physics equations",
        "doi": "10.1063/5.0157763",
        "year": 2023,
        "abstract": "<jats:p>Numerous physics theories are rooted in partial differential equations (PDEs). However, the increasingly intricate physics equations, especially those that lack analytic solutions or closed forms, have impeded the further development of physics. Computationally solving PDEs by classic numerical approaches suffers from the trade-off between accuracy and efficiency and is not applicable to the empirical data generated by unknown latent PDEs. To overcome this challenge, we present KoopmanLab, an efficient module of the Koopman neural operator (KNO) family, for learning PDEs without analytic solutions or closed forms. Our module consists of multiple variants of the KNO, a kind of mesh-independent neural-network-based PDE solvers developed following the dynamic system theory. The compact variants of KNO can accurately solve PDEs with small model sizes, while the large variants of KNO are more competitive in predicting highly complicated dynamic systems govern by unknown, high-dimensional, and non-linear PDEs. All variants are validated by mesh-independent and long-term prediction experiments implemented on representative PDEs (e.g., the Navier\u2013Stokes equation and the Bateman\u2013Burgers equation in fluid mechanics) and ERA5 (i.e., one of the largest high-resolution global-scale climate datasets in earth physics). These demonstrations suggest the potential of KoopmanLab to be a fundamental tool in diverse physics studies related to equations or dynamic systems.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Integrating uncertainty into deep learning models for enhanced prediction of nanocomposite materials\u2019 mechanical properties",
        "doi": "10.1063/5.0177062",
        "year": 2024,
        "abstract": "<jats:p>In this paper, we present a novel deep-learning framework that incorporates quantified uncertainty for predicting the mechanical properties of nanocomposite materials, specifically taking into account their morphology and composition. Due to the intricate microstructures of nanocomposites and their dynamic changes under diverse conditions, traditional methods, such as molecular dynamics simulations, often impose significant computational burdens. Our machine learning models, trained on comprehensive material datasets, provide a lower computational cost alternative, facilitating rapid exploration of design spaces and more reliable predictions. We employ both convolutional neural networks and feedforward neural networks for our predictions, training separate models for yield strength and ultimate tensile strength. Furthermore, we integrate uncertainty quantification into our models, thereby providing confidence intervals for our predictions and making them more reliable. This study paves the way for advancements in predicting the properties of nanocomposite materials and could potentially be expanded to cover a broad spectrum of materials in the future.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Accelerated and interpretable prediction of local properties in composites",
        "doi": "10.1063/5.0156517",
        "year": 2023,
        "abstract": "<jats:p>The localized stress and strain field simulation results are critical for understanding the mechanical properties of materials, such as strength and toughness. However, applying off-the-shelf machine learning or deep learning methods to a digitized microstructure restricts the image samples to be of a fixed size and also lacks interpretability. Additionally, existing methods that utilize deep learning models to solve boundary value problems require retraining the model for each set of boundary conditions. To address these limitations, we propose a customized Pixel-Wise Convolutional Neural Network (PWCNN) to make fast predictions of stress and strain fields pixel-by-pixel under different loading conditions and for a wide range of composite microstructures of any size (e.g., much larger or smaller than the sample on which the PWCNN is trained). Through numerical experiments, we show that our PWCNN model serves as an alternative approach to numerical solution methods, such as finite element analysis, but is computationally more efficient, and the prediction errors on the test microstructure are around 5%. Moreover, we also propose an interpretable machine learning framework to facilitate the scientific discovery of why certain microstructures have better or worse performance than others, which has important implications in the design of composite microstructures in advanced manufacturing.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Experiment-based deep learning approach for power allocation with a programmable metasurface",
        "doi": "10.1063/5.0184328",
        "year": 2023,
        "abstract": "<jats:p>Metasurfaces designed with deep learning approaches have emerged as efficient tools for manipulating electromagnetic waves to achieve beam steering and power allocation objectives. However, the effects of complex environmental factors like obstacle blocking and other unavoidable scattering need to be sufficiently considered for practical applications. In this work, we employ an experiment-based deep learning approach for programmable metasurface design to control powers delivered to specific locations generally with obstacle blocking. Without prior physical knowledge of the complex system, large sets of experimental data can be efficiently collected with a programmable metasurface to train a deep neural network (DNN). The experimental data can inherently incorporate complex factors that are difficult to include if only simulation data are used for training. Moreover, the DNN can be updated by collecting new experimental data on-site to adapt to changes in the environment. Our proposed experiment-based DNN demonstrates significant potential for intelligent wireless communication, imaging, sensing, and quiet-zone control for practical applications.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Bayesian optimization approach to quantify the effect of input parameter uncertainty on predictions of numerical physics simulations",
        "doi": "10.1063/5.0151747",
        "year": 2023,
        "abstract": "<jats:p>An understanding of how input parameter uncertainty in the numerical simulation of physical models leads to simulation output uncertainty is a challenging task. Common methods for quantifying output uncertainty, such as performing a grid or random search over the model input space, are computationally intractable for a large number of input parameters represented by a high-dimensional input space. It is, therefore, generally unclear as to whether a numerical simulation can reproduce a particular outcome (e.g., a set of experimental results) with a plausible set of model input parameters. Here, we present a method for efficiently searching the input space using Bayesian optimization to minimize the difference between the simulation output and a set of experimental results. Our method allows explicit evaluation of the probability that the simulation can reproduce the measured experimental results in the region of input space defined by the uncertainty in each input parameter. We apply this method to the simulation of charge-carrier dynamics in the perovskite semiconductor methyl-ammonium lead iodide (MAPbI3), which has attracted attention as a light harvesting material in solar cells. From our analysis, we conclude that the formation of large polarons, quasiparticles created by the coupling of excess electrons or holes with ionic vibrations, cannot explain the experimentally observed temperature dependence of electron mobility.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Autoregressive transformers for data-driven spatiotemporal learning of turbulent flows",
        "doi": "10.1063/5.0152212",
        "year": 2023,
        "abstract": "<jats:p>A convolutional encoder\u2013decoder-based transformer model is proposed for autoregressively training on spatiotemporal data of turbulent flows. The prediction of future fluid flow fields is based on the previously predicted fluid flow field to ensure long-term predictions without diverging. A combination of convolutional neural networks and transformer architecture is utilized to handle both the spatial and temporal dimensions of the data. To assess the performance of the model, a priori assessments are conducted, and significant agreements are found with the ground truth data. The a posteriori predictions, which are generated after a considerable number of simulation steps, exhibit predicted variances. The autoregressive training and prediction of a posteriori states are deemed crucial steps toward the development of more complex data-driven turbulence models and simulations. The highly nonlinear and chaotic dynamics of turbulent flows can be handled by the proposed model, and accurate predictions over long time horizons can be generated. Overall, the potential of using deep learning techniques to improve the accuracy and efficiency of turbulence modeling and simulation is demonstrated by this approach. The proposed model can be further optimized and extended to incorporate additional physics and boundary conditions, paving the way for more realistic simulations of complex fluid dynamics.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Prediction and control of spatiotemporal chaos by <i>learning</i> conjugate tubular neighborhoods",
        "doi": "10.1063/5.0181022",
        "year": 2024,
        "abstract": "<jats:p>I present a data-driven predictive modeling tool that is applicable to high-dimensional chaotic systems with unstable periodic orbits. The basic idea is using deep neural networks to learn coordinate transformations between the trajectories in the periodic orbits\u2019 neighborhoods and those of low-dimensional linear systems in a latent space. I argue that the resulting models are partially interpretable since their latent-space dynamics is fully understood. To illustrate the method, I apply it to the numerical solutions of the Kuramoto\u2013Sivashinsky partial differential equation in one dimension. Besides the forward-time predictions, I also show that these models can be leveraged for control.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Multiplexed gradient descent: Fast online training of modern datasets on hardware neural networks without backpropagation",
        "doi": "10.1063/5.0157645",
        "year": 2023,
        "abstract": "<jats:p>We present multiplexed gradient descent (MGD), a gradient descent framework designed to easily train analog or digital neural networks in hardware. MGD utilizes zero-order optimization techniques for online training of hardware neural networks. We demonstrate its ability to train neural networks on modern machine learning datasets, including CIFAR-10 and Fashion-MNIST, and compare its performance to backpropagation. Assuming realistic timescales and hardware parameters, our results indicate that these optimization techniques can train a network on emerging hardware platforms orders of magnitude faster than the wall-clock time of training via backpropagation on a standard GPU, even in the presence of imperfect weight updates or device-to-device variations in the hardware. We additionally describe how it can be applied to existing hardware as part of chip-in-the-loop training or integrated directly at the hardware level. Crucially, because the MGD framework is model-free it can be applied to nearly any hardware platform with tunable parameters, and its gradient descent process can be optimized to compensate for specific hardware limitations, such as slow parameter-update speeds or limited input bandwidth.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Improved prediction for failure time of multilayer ceramic capacitors (MLCCs): A physics-based machine learning approach",
        "doi": "10.1063/5.0158360",
        "year": 2023,
        "abstract": "<jats:p>Multilayer ceramic capacitors (MLCC) play a vital role in electronic systems, and their reliability is of critical importance. The ongoing advancement in MLCC manufacturing has improved capacitive volumetric density for both low and high voltage devices; however, concerns about long-term stability under higher fields and temperatures are always a concern, which impact their reliability and lifespan. Consequently, predicting the mean time to failure (MTTF) for MLCCs remains a challenge due to the limitations of existing models. In this study, we develop a physics-based machine learning approach using the eXtreme Gradient Boosting method to predict the MTTF of X7R MLCCs under various temperature and voltage conditions. We employ a transfer learning framework to improve prediction accuracy for test conditions with limited data and to provide predictions for test conditions where no experimental data exists. We compare our model with the conventional Eyring model (EM) and, more recently, the tipping point model (TPM) in terms of accuracy and performance. Our results show that the machine learning model consistently outperforms both the EM and TPM, demonstrating superior accuracy and stability across different conditions. Our model also exhibits a reliable performance for untested voltage and temperature conditions, making it a promising approach for predicting MTTF in MLCCs.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Manifold projection image segmentation for nano-XANES imaging",
        "doi": "10.1063/5.0167584",
        "year": 2023,
        "abstract": "<jats:p>As spectral imaging techniques are becoming more prominent in science, advanced image segmentation algorithms are required to identify appropriate domains in these images. We present a version of image segmentation called manifold projection image segmentation (MPIS) that is generally applicable to a broad range of systems without the need for training because MPIS uses unsupervised machine learning with a few physically motivated hyperparameters. We apply MPIS to nanoscale x-ray absorption near edge structure (XANES) imaging, where XANES spectra are collected with nanometer spatial resolution. We show the superiority of manifold projection over linear transformations, such as the commonly used principal component analysis (PCA). Moreover, MPIS maintains accuracy while reducing computation time and sensitivity to noise compared to the standard nano-XANES imaging analysis procedure. Finally, we demonstrate how multimodal information, such as x-ray fluorescence data and spatial location of pixels, can be incorporated into the MPIS framework. We propose that MPIS is adaptable for any spectral imaging technique, including scanning transmission x-ray microscopy, where the length scale of domains is larger than the resolution of the experiment.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "Identification of novel organic polar materials: A machine learning study with importance sampling",
        "doi": "10.1063/5.0162380",
        "year": 2023,
        "abstract": "<jats:p>Recent advances in the synthesis of polar molecular materials have produced practical alternatives to ferroelectric ceramics, opening up exciting new avenues for their incorporation into modern electronic devices. However, in order to realize the full potential of polar polymer and molecular crystals for modern technological applications, it is paramount to assemble and evaluate all the available data for such compounds, identifying descriptors that could be associated with an emergence of ferroelectricity. In this paper, we utilized data-driven approaches to judiciously shortlist candidate materials from a wide chemical space that could possess ferroelectric functionalities. A machine learning study with importance sampling was employed to address the challenge of having a limited amount of available data on already-known organic ferroelectrics. Sets of molecular- and crystal-level descriptors were combined with a Random Forest Regression algorithm in order to predict the spontaneous polarization of the shortlisted compounds. First-principles simulations were performed to further validate the predictions obtained from the machine learning model.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Contextual beamforming: Exploiting location and AI for enhanced wireless telecommunication performance",
        "doi": "10.1063/5.0176422",
        "year": 2024,
        "abstract": "<jats:p>Beamforming, an integral component of modern mobile networks, enables spatial selectivity and improves network quality. However, many beamforming techniques are iterative, introducing unwanted latency to the system. In recent times, there has been a growing interest in leveraging mobile users\u2019 location information to expedite beamforming processes. This paper explores the concept of contextual beamforming, discussing its advantages, disadvantages, and implications. Notably, we demonstrate an impressive 53% improvement in the signal-to-interference-plus-noise ratio by implementing the adaptive beamforming maximum ratio transmission (MRT) algorithm compared to scenarios without beamforming. It further elucidates how MRT contributes to contextual beamforming. The importance of localization in implementing contextual beamforming is also examined. Additionally, the paper delves into the use of artificial intelligence (AI) schemes, including machine learning and deep learning, in implementing contextual beamforming techniques that leverage user location information. Based on the comprehensive review, the results suggest that the combination of MRT and zero-forcing techniques, alongside deep neural networks employing Bayesian optimization, represents the most promising approach for contextual beamforming. Furthermore, the study discusses the future potential of programmable switches, such as Tofino\u2014an innovative switch developed by Barefoot Networks (now a part of Intel)\u2014in enabling location-aware beamforming. This paper highlights the significance of contextual beamforming for improving wireless telecommunications performance. By capitalizing on location information and employing advanced AI techniques, the field can overcome challenges and unlock new possibilities for delivering reliable and efficient mobile networks.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Harnessing nonlinear conductive characteristic of TiO2/HfO2 memristor crossbar for implementing parallel vector\u2013matrix multiplication",
        "doi": "10.1063/5.0195190",
        "year": 2024,
        "abstract": "<jats:p>Memristor crossbar arrays are expected to achieve highly energy-efficient neuromorphic computing via implementing parallel vector\u2013matrix multiplication (VMM) in situ. The similarities between memristors and neural synapses offer opportunities for realizing hardware-based brain-inspired computing, such as spike neural networks. However, the nonlinear I\u2013V characteristics of the memristors limit the implementation of parallel VMM on passive memristor crossbar arrays. In our work, we propose to utilize differential conductance as a synaptic weight to implement linear VMM operations on a passive memristor array in parallel. We fabricated a TiO2/HfO2 memristor crossbar array, in which differential-conductance-based synaptic weight exhibits plasticity, nonvolatility, multi-states, and tunable ON/OFF ratio. The noise-dependent accuracy performance of VMM operations based on the proposed approach was evaluated, offering an optimization guideline. Furthermore, we demonstrated a spike neural network circuit capable of processing small spiking signals through the differential-conductance-based synapses. The experimental results showcase effective space-coded and time-coded spike pattern recognition. Importantly, our work opens up new possibilities for the development of passive memristor arrays, leading to increased energy and area efficiency in brain-inspired chips.</jats:p>",
        "is_referenced_by_count": 0
    },
    {
        "title": "Unsupervised machine learning discovery of structural units and transformation pathways from imaging data",
        "doi": "10.1063/5.0147316",
        "year": 2023,
        "abstract": "<jats:p>We show that unsupervised machine learning can be used to learn chemical transformation pathways from observational Scanning Transmission Electron Microscopy (STEM) data. To enable this analysis, we assumed the existence of atoms, a discreteness of atomic classes, and the presence of an explicit relationship between the observed STEM contrast and the presence of atomic units. With only these postulates, we developed a machine learning method leveraging a rotationally invariant variational autoencoder (VAE) that can identify the existing molecular fragments observed within a material. The approach encodes the information contained in STEM image sequences using a small number of latent variables, allowing the exploration of chemical transformation pathways by tracing the evolution of atoms in the latent space of the system. The results suggest that atomically resolved STEM data can be used to derive fundamental physical and chemical mechanisms involved, by providing encodings of the observed structures that act as bottom-up equivalents of structural order parameters. The approach also demonstrates the potential of variational (i.e., Bayesian) methods in the physical sciences and will stimulate the development of more sophisticated ways to encode physical constraints in the encoder\u2013decoder architectures and generative physical laws and causal relationships in the latent space of VAEs.</jats:p>",
        "is_referenced_by_count": 2
    },
    {
        "title": "In-memory and in-sensor reservoir computing with memristive devices",
        "doi": "10.1063/5.0174863",
        "year": 2024,
        "abstract": "<jats:p>Despite the significant progress made in deep learning on digital computers, their energy consumption and computational speed still fall short of meeting the standards for brain-like computing. To address these limitations, reservoir computing (RC) has been gaining increasing attention across communities of electronic devices, computing systems, and machine learning, notably with its in-memory or in-sensor implementation on the hardware\u2013software co-design. Hardware regarded, in-memory or in-sensor computers leverage emerging electronic and optoelectronic devices for data processing right where the data are stored or sensed. This technology dramatically reduces the energy consumption from frequent data transfers between sensing, storage, and computational units. Software regarded, RC enables real-time edge learning thanks to its brain-inspired dynamic system with massive training complexity reduction. From this perspective, we survey recent advancements in in-memory/in-sensor RC, including algorithm designs, material and device development, and downstream applications in classification and regression problems, and discuss challenges and opportunities ahead in this emerging field.</jats:p>",
        "is_referenced_by_count": 1
    },
    {
        "title": "Resistance transient dynamics in switchable perovskite memristors",
        "doi": "10.1063/5.0153289",
        "year": 2023,
        "abstract": "<jats:p>Memristor devices have been investigated for their properties of resistive modulation that can be used in data storage and brain-like computation elements as artificial synapses and neurons. Memristors are characterized by an onset of high current values under applied voltage that produces a transition to a low resistance state or successively to different stable states of increasing conductivity that implement synaptic weights. Here, we develop a nonlinear model to explain the variation with time of the voltage and the resistance and compare it to experimental results on ionic\u2013electronic halide perovskite memristors. We find separate experimental signatures of the capacitive discharge and inductive current increase. We show that the capacitor produces an increase step of the resistance due to the influence of the series resistance. In contrast, the inductor feature associated with inverted hysteresis causes a decrease of the resistance, as observed experimentally. The chemical inductor feature dominates the potentiation effect in which the conductivity increases with the voltage stimulus. Our results enable a quantitative characterization of highly nonlinear electronic devices using a combination of techniques such as time transient decays and impedance spectroscopy.</jats:p>",
        "is_referenced_by_count": 9
    }
]